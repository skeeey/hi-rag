{"docstore/metadata": {"9588cdb7-f2f3-4d46-af52-1f0d665b2d40": {"doc_hash": "33d3834305f6e6493f082ae00e28150a84531828a4af89e9d5f19f5e51225c18"}, "ad849a43-241e-400e-9bff-6627d6e6ec58": {"doc_hash": "09de04865066fda596a684bad9cc5b7b3381c6f4d87e6c30325fe19c8025836d"}, "fcce6751-3e0e-438c-ba36-913dc9f9c41e": {"doc_hash": "c69b4241f4fe91453399c6113ce00e58a534d1b0aa62b2fdc6e62d914c0cdddc"}, "35525858-3ccc-403d-a577-001b1a567924": {"doc_hash": "b805dfb302b062b32d22f43a36ea80aa0ef3e2c4a8be34749f34120141b4cc7c"}, "0aabf701-4c83-4b6f-926f-3286a65b2419": {"doc_hash": "770a3e29bc985927e3cedf6887344d0f921ed42a3f026f9db92e8132f5952c52"}, "7de721a4-671c-4833-b793-889701330075": {"doc_hash": "a0775d45fb8e13f8bf01ad5c753676c12f37428d8144cf6fc3c8b4fe5b592f60"}, "378dbcc3-837f-465f-8e96-c5732b85de38": {"doc_hash": "b870057c2bc2066fa622bb48dcdb8d23c00de57786102f85aba851c32ef3d091"}, "d39d55fe-ee32-4125-91d1-a48c844bf232": {"doc_hash": "00c2bf588c773a4a346bf3a5e2e8171f89d23f61a25bbbdd16e0d11f739ee5b5"}, "fc2fdffb-6777-43f9-b5fd-4206697c876a": {"doc_hash": "c297c5f2b61fe47922e9a8987dc0f57aa337cea2b2718427b49efa1fd87ad003"}, "5ee83bd6-5651-4808-91b6-3690152713da": {"doc_hash": "3a796c13924d927387f788a104005ba5ea74ae198c754d97140a580c1cfa69a5"}, "fe93d5e1-9887-4c49-a202-0e99dae0ecb6": {"doc_hash": "f416c6a85cb7b392d6649659e2fb5a8e69a5c94778f6e4cf4a2a48cf5ca1ed3c"}, "25035d50-503f-4019-92e5-8d4244d9fc31": {"doc_hash": "00dc6b9c77ffdfcb1f41fe02469e2c46c775faf6d6ce80e9f23d95fad97fb84b"}, "3fe02681-cd77-44dc-9cd3-c51406f0c13b": {"doc_hash": "ccf5531fcebad4f70186851ef3a6aa6c9dec0a6730e3fd55d751bc48062a4b1e"}, "9ac7f74d-d144-4bfe-b82d-a58c76ef80eb": {"doc_hash": "06b520fff85cea612d5729fd47607eaf9cd794dff275720e44c104b9c6cf3e4b"}, "79336ac5-934d-435c-8a16-69634e5fc244": {"doc_hash": "50fc32ab06471d8a851e3708da8681a67b1641c8500c1ac03804170c0986cf43"}, "7576b933-dde7-4076-a724-43dbad0db089": {"doc_hash": "b3d0b125fb6f5bbbca65cd9c62ccaf4aa7ca2eef034d4bc3cf2ce1d0cb7f3fba"}, "05fecf46-7952-4cf4-9978-2e61face2f01": {"doc_hash": "cf339199ffc7a9cecaf7bfc28a184266dae6a586a3a67a68acec84c1409641a6"}, "15068f1a-bf98-477c-9f82-18f02d133f53": {"doc_hash": "a25fa85d4a9a527de8e3f4318103b1bdbeed5d2916d7a675d8b752ccd62d8ee9"}, "38466272-90c3-4529-9605-f79180422b6b": {"doc_hash": "bedcf11d18f955ad745b8826a0cbf3166823a087282818ddf4cd371d1c613ee6"}, "6ee7fa24-2fe2-4836-8fba-62aab1276a4c": {"doc_hash": "e3a9bac30186eb6219f845d6900196ac9ee1cc028891a98559a4fcc19ed1df4e"}, "d383f5fe-c49c-41b8-bd0c-9c611eaf3204": {"doc_hash": "fbe51a640e6bee47e775222d8012362c848932fa1d7f1d66b1830cbaf1f3aa72"}, "bc98ad6f-e751-4ad6-9e92-65875c0be89f": {"doc_hash": "942fcf27ef06f3765f5d0f39e097e4aa6fc32c524c78ab157074c156e50ad4bb"}, "08a3bfd8-a1d3-4791-aa06-4cce0c59db37": {"doc_hash": "4facddaf8f6a959c0d07b225ab4107405aaaded17bd7d91def669f9fc4de3e7c"}, "210f6cf2-7cad-4c99-af9c-b15241620d9e": {"doc_hash": "6b828f568cdcc19281e9e59cc67c111b57f29c1f7a0bc6f4e3113058f604a9f1"}, "df687014-0152-4bb6-a745-f573a02f0c8c": {"doc_hash": "abe9c7b032ad87589f8922fd2721454063b4e654c931ec857913286ff98a5aa3"}, "29fecb31-b7ba-43fa-94f6-aae61f96e599": {"doc_hash": "81921972f2f23684906dfabc9556dfc9520f7e8bd3766daee8458efaa37a6943"}, "dd226be5-5385-4062-bc26-230467c3f604": {"doc_hash": "4b4a99c380f72567262237b6306c4eda4dcd1d3949235c1c867982fdb3c6b829"}, "a9da59e6-ce38-4cdb-aba8-0ee4a3ebcf30": {"doc_hash": "161c8eab8c4f75bebf5496cd12d30472464a697a1054a1629b255e72bc1e5a20"}, "62d7f706-6a87-4ffb-bef9-57f43469cf87": {"doc_hash": "3f6e2b9135c3bc30af6e0e59a68fed82b7b342536396b7ce3b02e8454e719d7c"}, "1ecd8271-b8b8-4c97-aece-bf328cac961d": {"doc_hash": "87911dbe969f0dcdf0e61c899edc02a98c3d882aa20a545eb1f2c3dd5aa8d0ea"}, "a5d7308c-695f-43e1-b065-e354ceadc846": {"doc_hash": "6ef8ebcbd09d0c9de3188216e1e10d26e55c4355a0ccd45c8cfcb17f6f0307f0"}, "8196e504-6b96-4f71-9900-9577678576d2": {"doc_hash": "5cdae6c448b81c9bfee4f5db6f3d9f8038941a5b221277e7162fe889b4f5433f"}, "18d63416-8aa1-49f8-be89-a160ced2e4a8": {"doc_hash": "5ceb0107fd284790838bc343cc7523461924e2eb10cb712da09ba02a1ad83718"}, "98b38f52-7ea3-47ce-90a5-8675baca361b": {"doc_hash": "21085fc25bae1e43cdfbb90ab67834af7a4cde60b0cdc9bf88b2c32ec2d859ef"}, "7420811d-bd3f-432c-8ddc-f5d9279985ba": {"doc_hash": "f10e93b7dc5f2fbb7b3a5efe026ebc622da0415504e5627cf84cb4ac34e2df71"}, "38ed601c-c93d-4226-9d3c-259a036dfbf5": {"doc_hash": "30459dea3fd7f90312911a76d5988c1db10d725fb6ee75b140183c03ff108b0f"}, "62c1d69c-dc87-44d1-b4ff-98e3c659f7cc": {"doc_hash": "50c61b423fc46fc1fb69b3e858188d7bc03f2714ccf6306f7034e64a9942fd1f"}, "8b7630b8-439f-47ec-afa9-804e7bafe506": {"doc_hash": "7f889c377e46b2d76292bf8cc5c53e170b71a30eaf581b23a0d96707572b2568"}, "3edc0a60-c204-4f81-a74f-f1346c63a277": {"doc_hash": "155743ceb9c718b74fadf63d1a52f04ada86ba3628924050ef2d9063545c404c"}, "8b944b41-b771-4ce2-bc6b-c529f641b6a8": {"doc_hash": "d89a3112248c562f48a602e05ff8d03366dc406cce486fe85d34e8f564b0a223"}, "5590a902-cd61-4b45-b297-95b231551518": {"doc_hash": "a5f0626e9957afbb12adb8bffd1b82d20803d6d8db82af073be9d696984cf579"}, "80137291-2a05-437d-b53b-54d977631b94": {"doc_hash": "9b000f96660eacfa65db7d34ada173ae7ed8073d7d58fdcd3e34a9a5bd2d9a54"}, "702be77f-6a9e-4cbb-b89c-abe017392e29": {"doc_hash": "a4f3f62535a8ba5c21012d86a8e6c5ac406a3c27ab1b746ad066655a5564fb96"}, "dac72528-7431-413f-bdd1-421873db0bb1": {"doc_hash": "efe38d77599d15434e6c8ea8bd15671987170de8813c471befdaef4599ceae75"}, "d836d318-3fb1-41b4-9eb8-ac2ad3875f99": {"doc_hash": "83ab42f74557a3711368057fccccce97b2a635a619773566eed671452b92e9ee"}, "2322568a-0e51-42eb-8abc-ba38c2a961f6": {"doc_hash": "1bc0830e88a5fcae2abf41394647fa7cd27466e19cdfa8c70949031abb4dbf87"}, "fc6e9d4f-4023-4a6d-8c9f-e3524bf97d5d": {"doc_hash": "9b85f6744aad298f00f110374bc0666af0bd927960568bed56486bd96252173b"}, "cad615e2-1cf3-40c2-be28-65d0c21deb1c": {"doc_hash": "33d3834305f6e6493f082ae00e28150a84531828a4af89e9d5f19f5e51225c18", "ref_doc_id": "9588cdb7-f2f3-4d46-af52-1f0d665b2d40"}, "19e616ee-a1c3-4102-aa6a-0daa99f212b0": {"doc_hash": "09de04865066fda596a684bad9cc5b7b3381c6f4d87e6c30325fe19c8025836d", "ref_doc_id": "ad849a43-241e-400e-9bff-6627d6e6ec58"}, "22f516ba-81b7-4d30-b0b2-df07ee8cbdea": {"doc_hash": "c69b4241f4fe91453399c6113ce00e58a534d1b0aa62b2fdc6e62d914c0cdddc", "ref_doc_id": "fcce6751-3e0e-438c-ba36-913dc9f9c41e"}, "2ec500ef-febf-4cdd-8ef1-4d9c2da75abd": {"doc_hash": "b805dfb302b062b32d22f43a36ea80aa0ef3e2c4a8be34749f34120141b4cc7c", "ref_doc_id": "35525858-3ccc-403d-a577-001b1a567924"}, "c6d14741-d609-4fae-8015-3b863212447f": {"doc_hash": "34d976e4423f262147493f1bce805fcab4bb43b60940876244d9d358d79d7347", "ref_doc_id": "0aabf701-4c83-4b6f-926f-3286a65b2419"}, "79685708-ae8b-45e8-be44-6dda42f18f17": {"doc_hash": "8b53e80116fbb8de6a8dda839439ae3a3c79c66bda5a57da0698c4b043e9c62c", "ref_doc_id": "0aabf701-4c83-4b6f-926f-3286a65b2419"}, "01244469-7df9-4bce-9dbc-d53a0a3b7a09": {"doc_hash": "3702151f8cb1fb0bdd21e5ffc6aba9f4d6a28fd56300363bbba63815012e4b12", "ref_doc_id": "7de721a4-671c-4833-b793-889701330075"}, "e8627330-4131-45ca-804c-4d3bd1567b74": {"doc_hash": "87ee09df68efd305bf384d030b5a55ab6dc31fe2a01f5c63b3b70195a88da9bb", "ref_doc_id": "7de721a4-671c-4833-b793-889701330075"}, "a8ab4073-7432-4091-9796-017b329c6ed6": {"doc_hash": "5464eed4dfebb3420ffb9af04f2a7e8acec6105d08be415a458aa9d1db074b03", "ref_doc_id": "378dbcc3-837f-465f-8e96-c5732b85de38"}, "2b38aa4a-304b-4c42-94a7-11d989962f66": {"doc_hash": "786593439ff10b80a37445c76aa3e172f2435e1b869683e5ef1844c81f000f01", "ref_doc_id": "378dbcc3-837f-465f-8e96-c5732b85de38"}, "25148466-216d-4c24-b5e3-44153e8a90d7": {"doc_hash": "00c2bf588c773a4a346bf3a5e2e8171f89d23f61a25bbbdd16e0d11f739ee5b5", "ref_doc_id": "d39d55fe-ee32-4125-91d1-a48c844bf232"}, "e1e03637-5d3a-4e7d-bd97-5b0ccf50e1bb": {"doc_hash": "c297c5f2b61fe47922e9a8987dc0f57aa337cea2b2718427b49efa1fd87ad003", "ref_doc_id": "fc2fdffb-6777-43f9-b5fd-4206697c876a"}, "2b4b2e8d-520c-4b74-85e1-278d1f44c84e": {"doc_hash": "3a796c13924d927387f788a104005ba5ea74ae198c754d97140a580c1cfa69a5", "ref_doc_id": "5ee83bd6-5651-4808-91b6-3690152713da"}, "d240cfa0-7084-4ea0-b329-b6dd7ad61798": {"doc_hash": "f416c6a85cb7b392d6649659e2fb5a8e69a5c94778f6e4cf4a2a48cf5ca1ed3c", "ref_doc_id": "fe93d5e1-9887-4c49-a202-0e99dae0ecb6"}, "167ac1ab-ca20-4bbd-812c-e3e47dc83686": {"doc_hash": "00dc6b9c77ffdfcb1f41fe02469e2c46c775faf6d6ce80e9f23d95fad97fb84b", "ref_doc_id": "25035d50-503f-4019-92e5-8d4244d9fc31"}, "cd7454dd-baec-4aef-a45e-c617c6bf8069": {"doc_hash": "ccf5531fcebad4f70186851ef3a6aa6c9dec0a6730e3fd55d751bc48062a4b1e", "ref_doc_id": "3fe02681-cd77-44dc-9cd3-c51406f0c13b"}, "cdb53133-cc84-4253-9043-70dcd60c421d": {"doc_hash": "06b520fff85cea612d5729fd47607eaf9cd794dff275720e44c104b9c6cf3e4b", "ref_doc_id": "9ac7f74d-d144-4bfe-b82d-a58c76ef80eb"}, "2e752e4e-7153-48fe-88e7-73240050b054": {"doc_hash": "50fc32ab06471d8a851e3708da8681a67b1641c8500c1ac03804170c0986cf43", "ref_doc_id": "79336ac5-934d-435c-8a16-69634e5fc244"}, "5c761ea2-ac52-4827-b20f-83b8ebc1ade4": {"doc_hash": "b3d0b125fb6f5bbbca65cd9c62ccaf4aa7ca2eef034d4bc3cf2ce1d0cb7f3fba", "ref_doc_id": "7576b933-dde7-4076-a724-43dbad0db089"}, "270ebb66-2cda-4460-8431-67c954bb91ab": {"doc_hash": "cf339199ffc7a9cecaf7bfc28a184266dae6a586a3a67a68acec84c1409641a6", "ref_doc_id": "05fecf46-7952-4cf4-9978-2e61face2f01"}, "b0310e71-4439-45d2-8667-113f5691e793": {"doc_hash": "a25fa85d4a9a527de8e3f4318103b1bdbeed5d2916d7a675d8b752ccd62d8ee9", "ref_doc_id": "15068f1a-bf98-477c-9f82-18f02d133f53"}, "3f41c79b-efbb-4768-b338-aae492905cd1": {"doc_hash": "bedcf11d18f955ad745b8826a0cbf3166823a087282818ddf4cd371d1c613ee6", "ref_doc_id": "38466272-90c3-4529-9605-f79180422b6b"}, "d9d072f3-56a7-47c1-a808-4a4ae658ea26": {"doc_hash": "e3a9bac30186eb6219f845d6900196ac9ee1cc028891a98559a4fcc19ed1df4e", "ref_doc_id": "6ee7fa24-2fe2-4836-8fba-62aab1276a4c"}, "9e7773a0-93c1-4d79-82f6-d29464867d6f": {"doc_hash": "fbe51a640e6bee47e775222d8012362c848932fa1d7f1d66b1830cbaf1f3aa72", "ref_doc_id": "d383f5fe-c49c-41b8-bd0c-9c611eaf3204"}, "79c2abe9-351c-4c80-990c-928442cd7fb2": {"doc_hash": "942fcf27ef06f3765f5d0f39e097e4aa6fc32c524c78ab157074c156e50ad4bb", "ref_doc_id": "bc98ad6f-e751-4ad6-9e92-65875c0be89f"}, "2495292d-46f8-4ab6-bf8c-1169511f9b7e": {"doc_hash": "4facddaf8f6a959c0d07b225ab4107405aaaded17bd7d91def669f9fc4de3e7c", "ref_doc_id": "08a3bfd8-a1d3-4791-aa06-4cce0c59db37"}, "0b8d2fa4-4b70-4b1e-ac25-6075d86f376a": {"doc_hash": "6b828f568cdcc19281e9e59cc67c111b57f29c1f7a0bc6f4e3113058f604a9f1", "ref_doc_id": "210f6cf2-7cad-4c99-af9c-b15241620d9e"}, "2b7ee835-ddec-44f7-98c9-89c8ac94a2e6": {"doc_hash": "abe9c7b032ad87589f8922fd2721454063b4e654c931ec857913286ff98a5aa3", "ref_doc_id": "df687014-0152-4bb6-a745-f573a02f0c8c"}, "753f7f61-a993-4eeb-8e71-cf99c152f5d2": {"doc_hash": "81921972f2f23684906dfabc9556dfc9520f7e8bd3766daee8458efaa37a6943", "ref_doc_id": "29fecb31-b7ba-43fa-94f6-aae61f96e599"}, "a16e535e-f070-46e7-8877-d02518b7eb91": {"doc_hash": "4b4a99c380f72567262237b6306c4eda4dcd1d3949235c1c867982fdb3c6b829", "ref_doc_id": "dd226be5-5385-4062-bc26-230467c3f604"}, "9dbcbebd-4af0-486a-b73d-d9623a8b3ee2": {"doc_hash": "161c8eab8c4f75bebf5496cd12d30472464a697a1054a1629b255e72bc1e5a20", "ref_doc_id": "a9da59e6-ce38-4cdb-aba8-0ee4a3ebcf30"}, "be8fabb0-75ab-4fd0-a051-7bfafec82d32": {"doc_hash": "3f6e2b9135c3bc30af6e0e59a68fed82b7b342536396b7ce3b02e8454e719d7c", "ref_doc_id": "62d7f706-6a87-4ffb-bef9-57f43469cf87"}, "eb560299-f2ae-4b86-9127-e2a946003fd4": {"doc_hash": "87911dbe969f0dcdf0e61c899edc02a98c3d882aa20a545eb1f2c3dd5aa8d0ea", "ref_doc_id": "1ecd8271-b8b8-4c97-aece-bf328cac961d"}, "0c7ad3c2-da84-41a2-9b9e-f689560c466d": {"doc_hash": "6ef8ebcbd09d0c9de3188216e1e10d26e55c4355a0ccd45c8cfcb17f6f0307f0", "ref_doc_id": "a5d7308c-695f-43e1-b065-e354ceadc846"}, "c7602244-50b6-43af-90f9-d486e0d84232": {"doc_hash": "5cdae6c448b81c9bfee4f5db6f3d9f8038941a5b221277e7162fe889b4f5433f", "ref_doc_id": "8196e504-6b96-4f71-9900-9577678576d2"}, "267d064f-3ed4-4e37-a7d2-8f01f11ee89c": {"doc_hash": "5ceb0107fd284790838bc343cc7523461924e2eb10cb712da09ba02a1ad83718", "ref_doc_id": "18d63416-8aa1-49f8-be89-a160ced2e4a8"}, "35683573-feb1-444b-b76d-0364e0af53fe": {"doc_hash": "21085fc25bae1e43cdfbb90ab67834af7a4cde60b0cdc9bf88b2c32ec2d859ef", "ref_doc_id": "98b38f52-7ea3-47ce-90a5-8675baca361b"}, "11dc20d7-a40f-4ce1-adfc-6fba5ce7badb": {"doc_hash": "f10e93b7dc5f2fbb7b3a5efe026ebc622da0415504e5627cf84cb4ac34e2df71", "ref_doc_id": "7420811d-bd3f-432c-8ddc-f5d9279985ba"}, "8561eb5d-de5f-425d-9e98-31baea6b7d47": {"doc_hash": "30459dea3fd7f90312911a76d5988c1db10d725fb6ee75b140183c03ff108b0f", "ref_doc_id": "38ed601c-c93d-4226-9d3c-259a036dfbf5"}, "13e7ff9a-94d8-449f-ab2a-5d5bcff742de": {"doc_hash": "50c61b423fc46fc1fb69b3e858188d7bc03f2714ccf6306f7034e64a9942fd1f", "ref_doc_id": "62c1d69c-dc87-44d1-b4ff-98e3c659f7cc"}, "e1896bc7-71d6-47be-b640-f9d50c6fa7fe": {"doc_hash": "7f889c377e46b2d76292bf8cc5c53e170b71a30eaf581b23a0d96707572b2568", "ref_doc_id": "8b7630b8-439f-47ec-afa9-804e7bafe506"}, "b70af8eb-46fe-4794-bb54-f2b6a1d67e93": {"doc_hash": "155743ceb9c718b74fadf63d1a52f04ada86ba3628924050ef2d9063545c404c", "ref_doc_id": "3edc0a60-c204-4f81-a74f-f1346c63a277"}, "0f0e8304-1fa6-402e-9ab8-fb75afdc30c7": {"doc_hash": "d89a3112248c562f48a602e05ff8d03366dc406cce486fe85d34e8f564b0a223", "ref_doc_id": "8b944b41-b771-4ce2-bc6b-c529f641b6a8"}, "d795628d-b898-4be1-a204-bb47093f5136": {"doc_hash": "a5f0626e9957afbb12adb8bffd1b82d20803d6d8db82af073be9d696984cf579", "ref_doc_id": "5590a902-cd61-4b45-b297-95b231551518"}, "0d922663-f4db-4263-8d16-b05fdc73436c": {"doc_hash": "9b000f96660eacfa65db7d34ada173ae7ed8073d7d58fdcd3e34a9a5bd2d9a54", "ref_doc_id": "80137291-2a05-437d-b53b-54d977631b94"}, "1fb2e920-31e7-4d9e-a93d-50e7a1312a44": {"doc_hash": "a4f3f62535a8ba5c21012d86a8e6c5ac406a3c27ab1b746ad066655a5564fb96", "ref_doc_id": "702be77f-6a9e-4cbb-b89c-abe017392e29"}, "07ea2960-2d51-45aa-9840-5eb156a3b961": {"doc_hash": "efe38d77599d15434e6c8ea8bd15671987170de8813c471befdaef4599ceae75", "ref_doc_id": "dac72528-7431-413f-bdd1-421873db0bb1"}, "935c7e19-54e9-4d81-b1f0-6f9e645ae0c6": {"doc_hash": "83ab42f74557a3711368057fccccce97b2a635a619773566eed671452b92e9ee", "ref_doc_id": "d836d318-3fb1-41b4-9eb8-ac2ad3875f99"}, "2b8f6538-626f-49d2-82b8-1da59bcf939d": {"doc_hash": "1bc0830e88a5fcae2abf41394647fa7cd27466e19cdfa8c70949031abb4dbf87", "ref_doc_id": "2322568a-0e51-42eb-8abc-ba38c2a961f6"}, "ce20bc47-f608-4a56-9b0d-96fd35568b5b": {"doc_hash": "9b85f6744aad298f00f110374bc0666af0bd927960568bed56486bd96252173b", "ref_doc_id": "fc6e9d4f-4023-4a6d-8c9f-e3524bf97d5d"}}, "docstore/data": {"cad615e2-1cf3-40c2-be28-65d0c21deb1c": {"__data__": {"id_": "cad615e2-1cf3-40c2-be28-65d0c21deb1c", "embedding": null, "metadata": {"page_label": "1", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9588cdb7-f2f3-4d46-af52-1f0d665b2d40", "node_type": "4", "metadata": {"page_label": "1", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "33d3834305f6e6493f082ae00e28150a84531828a4af89e9d5f19f5e51225c18", "class_name": "RelatedNodeInfo"}}, "text": "Red Hat Advanced Cluster Management\nfor Kubernetes\n \n2.11\nTroubleshooting\nTroubleshooting\nLast Updated: 2024-07-23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 114, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "19e616ee-a1c3-4102-aa6a-0daa99f212b0": {"__data__": {"id_": "19e616ee-a1c3-4102-aa6a-0daa99f212b0", "embedding": null, "metadata": {"page_label": "2", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ad849a43-241e-400e-9bff-6627d6e6ec58", "node_type": "4", "metadata": {"page_label": "2", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "09de04865066fda596a684bad9cc5b7b3381c6f4d87e6c30325fe19c8025836d", "class_name": "RelatedNodeInfo"}}, "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "22f516ba-81b7-4d30-b0b2-df07ee8cbdea": {"__data__": {"id_": "22f516ba-81b7-4d30-b0b2-df07ee8cbdea", "embedding": null, "metadata": {"page_label": "3", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fcce6751-3e0e-438c-ba36-913dc9f9c41e", "node_type": "4", "metadata": {"page_label": "3", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "c69b4241f4fe91453399c6113ce00e58a534d1b0aa62b2fdc6e62d914c0cdddc", "class_name": "RelatedNodeInfo"}}, "text": "Red Hat Advanced Cluster Management for Kubernetes\n \n2.11\nTroubleshooting\nTroubleshooting", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 89, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ec500ef-febf-4cdd-8ef1-4d9c2da75abd": {"__data__": {"id_": "2ec500ef-febf-4cdd-8ef1-4d9c2da75abd", "embedding": null, "metadata": {"page_label": "4", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "35525858-3ccc-403d-a577-001b1a567924", "node_type": "4", "metadata": {"page_label": "4", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "b805dfb302b062b32d22f43a36ea80aa0ef3e2c4a8be34749f34120141b4cc7c", "class_name": "RelatedNodeInfo"}}, "text": "Legal Notice\nCopyright \n\u00a9\n 2024 Red Hat, Inc.\nThe text of and illustrations in this document are licensed by Red Hat under a Creative Commons\nAttribution\u2013Share Alike 3.0 Unported license (\"CC-BY-SA\"). An explanation of CC-BY-SA is\navailable at\nhttp://creativecommons.org/licenses/by-sa/3.0/\n. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must\nprovide the URL for the original version.\nRed Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert,\nSection 4d of CC-BY-SA to the fullest extent permitted by applicable law.\nRed Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift,\nFedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States\nand other countries.\nLinux \u00ae\n is the registered trademark of Linus Torvalds in the United States and other countries.\nJava \u00ae\n is a registered trademark of Oracle and/or its affiliates.\nXFS \u00ae\n is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States\nand/or other countries.\nMySQL \u00ae\n is a registered trademark of MySQL AB in the United States, the European Union and\nother countries.\nNode.js \u00ae\n is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the\nofficial Joyent Node.js open source or commercial project.\nThe \nOpenStack \u00ae\n Word Mark and OpenStack logo are either registered trademarks/service marks\nor trademarks/service marks of the OpenStack Foundation, in the United States and other\ncountries and are used with the OpenStack Foundation's permission. We are not affiliated with,\nendorsed or sponsored by the OpenStack Foundation, or the OpenStack community.\nAll other trademarks are the property of their respective owners.\nAbstract\nView a list of troubleshooting topics for your cluster. You can also use the must-gather command to\ncollect logs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1918, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c6d14741-d609-4fae-8015-3b863212447f": {"__data__": {"id_": "c6d14741-d609-4fae-8015-3b863212447f", "embedding": null, "metadata": {"page_label": "5", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0aabf701-4c83-4b6f-926f-3286a65b2419", "node_type": "4", "metadata": {"page_label": "5", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "770a3e29bc985927e3cedf6887344d0f921ed42a3f026f9db92e8132f5952c52", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79685708-ae8b-45e8-be44-6dda42f18f17", "node_type": "1", "metadata": {}, "hash": "c03bb17b212b841f9d076875f528ac0e2ae8fdad782c6481e882c28dde0254f7", "class_name": "RelatedNodeInfo"}}, "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nTable of Contents\nCHAPTER 1. TROUBLESHOOTING\n1.1. DOCUMENTED TROUBLESHOOTING\n1.2. RUNNING THE MUST-GATHER COMMAND TO TROUBLESHOOT\n1.2.1. Must-gather scenarios\n1.2.2. Must-gather procedure\n1.2.3. Must-gather in a disconnected environment\n1.2.4. Must-gather for a hosted cluster\n1.2.4.1. About the must-gather command for hosted clusters\n1.2.4.2. Prerequisites\n1.2.4.3. Entering the must-gather command for hosted clusters\n1.2.4.4. Entering the must-gather command in a disconnected environment\n1.2.4.5. Additional resources\n1.3. TROUBLESHOOTING INSTALLATION STATUS STUCK IN INSTALLING OR PENDING\n1.3.1. Symptom: Stuck in Pending status\n1.3.2. Resolving the problem: Adjust worker node sizing\n1.4. TROUBLESHOOTING REINSTALLATION FAILURE\n1.4.1. Symptom: Reinstallation failure\n1.4.2. Resolving the problem: Reinstallation failure\n1.5. TROUBLESHOOTING OCM-CONTROLLER ERRORS AFTER RED HAT ADVANCED CLUSTER\nMANAGEMENT UPGRADE\n1.5.1. Symptom: Troubleshooting ocm-controller errors after Red Hat Advanced Cluster Management upgrade\n1.5.2. Resolving the problem: Troubleshooting ocm-controller errors after Red Hat Advanced Cluster\nManagement upgrade\n1.5.2.1. Verification\n1.6. TROUBLESHOOTING AN OFFLINE CLUSTER\n1.6.1. Symptom: Cluster status is offline\n1.6.2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1812, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "79685708-ae8b-45e8-be44-6dda42f18f17": {"__data__": {"id_": "79685708-ae8b-45e8-be44-6dda42f18f17", "embedding": null, "metadata": {"page_label": "5", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0aabf701-4c83-4b6f-926f-3286a65b2419", "node_type": "4", "metadata": {"page_label": "5", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "770a3e29bc985927e3cedf6887344d0f921ed42a3f026f9db92e8132f5952c52", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6d14741-d609-4fae-8015-3b863212447f", "node_type": "1", "metadata": {"page_label": "5", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "34d976e4423f262147493f1bce805fcab4bb43b60940876244d9d358d79d7347", "class_name": "RelatedNodeInfo"}}, "text": "Resolving the problem: Adjust worker node sizing\n1.4. TROUBLESHOOTING REINSTALLATION FAILURE\n1.4.1. Symptom: Reinstallation failure\n1.4.2. Resolving the problem: Reinstallation failure\n1.5. TROUBLESHOOTING OCM-CONTROLLER ERRORS AFTER RED HAT ADVANCED CLUSTER\nMANAGEMENT UPGRADE\n1.5.1. Symptom: Troubleshooting ocm-controller errors after Red Hat Advanced Cluster Management upgrade\n1.5.2. Resolving the problem: Troubleshooting ocm-controller errors after Red Hat Advanced Cluster\nManagement upgrade\n1.5.2.1. Verification\n1.6. TROUBLESHOOTING AN OFFLINE CLUSTER\n1.6.1. Symptom: Cluster status is offline\n1.6.2. Resolving the problem: Cluster status is offline\n1.7. TROUBLESHOOTING A MANAGED CLUSTER IMPORT FAILURE\n1.7.1. Symptom: Imported cluster not available\n1.7.2. Resolving the problem: Imported cluster not available\n1.8. TROUBLESHOOTING CLUSTER WITH PENDING IMPORT STATUS\n1.8.1. Symptom: Cluster with pending import status\n1.8.2. Identifying the problem: Cluster with pending import status\n1.8.3. Resolving the problem: Cluster with pending import status\n1.9. TROUBLESHOOTING CLUSTER WITH ALREADY EXISTS ERROR\n1.9.1. Symptom: Already exists error log when importing OpenShift Container Platform cluster\n1.9.2. Identifying the problem: Already exists when importing OpenShift Container Platform cluster\n1.9.3. Resolving the problem: Already exists when importing OpenShift Container Platform cluster\n1.10. TROUBLESHOOTING CLUSTER CREATION ON VMWARE VSPHERE\n1.10.1. Managed cluster creation fails with certificate IP SAN error\n1.10.1.1. Symptom: Managed cluster creation fails with certificate IP SAN error\n1.10.1.2. Identifying the problem: Managed cluster creation fails with certificate IP SAN error\n1.10.1.3. Resolving the problem: Managed cluster creation fails with certificate IP SAN error\n1.10.2. Managed cluster creation fails with unknown certificate authority\n1.10.2.1. Symptom: Managed cluster creation fails with unknown certificate authority\n1.10.2.2. Identifying the problem: Managed cluster creation fails with unknown certificate authority\n1.10.2.3. Resolving the problem: Managed cluster creation fails with unknown certificate authority\n1.10.3. Managed cluster creation fails with expired certificate\n1.10.3.1. Symptom: Managed cluster creation fails with expired certificate\n1.10.3.2. Identifying the problem: Managed cluster creation fails with expired certificate\n5\n5\n6\n7\n7\n7\n8\n8\n8\n9\n9\n9\n9\n10\n10\n10\n10\n10\n11\n11\n11\n13\n13\n13\n13\n14\n14\n14\n15\n15\n15\n15\n15\n15\n16\n16\n16\n16\n17\n17\n17\n17\n17\n17\n17\n17\n17\n17\nTable of Contents\n1", "mimetype": "text/plain", "start_char_idx": 1202, "end_char_idx": 3742, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01244469-7df9-4bce-9dbc-d53a0a3b7a09": {"__data__": {"id_": "01244469-7df9-4bce-9dbc-d53a0a3b7a09", "embedding": null, "metadata": {"page_label": "6", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de721a4-671c-4833-b793-889701330075", "node_type": "4", "metadata": {"page_label": "6", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "a0775d45fb8e13f8bf01ad5c753676c12f37428d8144cf6fc3c8b4fe5b592f60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8627330-4131-45ca-804c-4d3bd1567b74", "node_type": "1", "metadata": {}, "hash": "06f4b7888a7e3b9ad28c969f1675d605900e8462532c54c1a5b4fbeb6e3f9870", "class_name": "RelatedNodeInfo"}}, "text": "1.10.3.3. Resolving the problem: Managed cluster creation fails with expired certificate\n1.10.4. Managed cluster creation fails with insufficient privilege for tagging\n1.10.4.1. Symptom: Managed cluster creation fails with insufficient privilege for tagging\n1.10.4.2. Identifying the problem: Managed cluster creation fails with insufficient privilege for tagging\n1.10.4.3. Resolving the problem: Managed cluster creation fails with insufficient privilege for tagging\n1.10.5. Managed cluster creation fails with invalid dnsVIP\n1.10.5.1. Symptom: Managed cluster creation fails with invalid dnsVIP\n1.10.5.2. Identifying the problem: Managed cluster creation fails with invalid dnsVIP\n1.10.5.3. Resolving the problem: Managed cluster creation fails with invalid dnsVIP\n1.10.6. Managed cluster creation fails with incorrect network type\n1.10.6.1. Symptom: Managed cluster creation fails with incorrect network type\n1.10.6.2. Identifying the problem: Managed cluster creation fails with incorrect network type\n1.10.6.3. Resolving the problem: Managed cluster creation fails with incorrect network type\n1.10.7. Managed cluster creation fails with an error processing disk changes\n1.10.7.1. Symptom: Adding the VMware vSphere managed cluster fails due to an error processing disk changes\n1.10.7.2. Identifying the problem: Adding the VMware vSphere managed cluster fails due to an error\nprocessing disk changes\n1.10.7.3. Resolving the problem: Adding the VMware vSphere managed cluster fails due to an error processing\ndisk changes\n1.11. MANAGED CLUSTER CREATION FAILS ON RED HAT OPENSTACK PLATFORM WITH UNKNOWN\nAUTHORITY ERROR\n1.11.1. Symptom: Managed cluster creation fails with unknown authority error\n1.11.2. Identifying the problem: Managed cluster creation fails with unknown authority error\n1.11.3. Resolving the problem: Managed cluster creation fails with unknown authority error\n1.12. TROUBLESHOOTING OPENSHIFT CONTAINER PLATFORM VERSION 3.11 CLUSTER IMPORT FAILURE\n1.12.1. Symptom: OpenShift Container Platform version 3.11 cluster import failure\n1.12.2. Identifying the problem: OpenShift Container Platform version 3.11 cluster import failure\n1.12.3. Resolving the problem: OpenShift Container Platform version 3.11 cluster import failure\n1.13. TROUBLESHOOTING IMPORTED CLUSTERS OFFLINE AFTER CERTIFICATE CHANGE\n1.13.1. Symptom: Clusters offline after certificate change\n1.13.2. Identifying the problem: Clusters offline after certificate change\n1.13.3. Resolving the problem: Clusters offline after certificate change\n1.14. NAMESPACE REMAINS AFTER DELETING A CLUSTER\n1.14.1. Symptom: Namespace remains after deleting a cluster\n1.14.2. Resolving the problem: Namespace remains after deleting a cluster\n1.15. AUTO-IMPORT-SECRET-EXISTS ERROR WHEN IMPORTING A CLUSTER\n1.15.1. Symptom: Auto import secret exists error when importing a cluster\n1.15.2. Resolving the problem: Auto-import-secret-exists error when importing a cluster\n1.16. TROUBLESHOOTING THE CINDER CONTAINER STORAGE INTERFACE (CSI) DRIVER FOR VOLSYNC\n1.16.1. Symptom: Volumesnapshot error state\n1.16.2. Resolving the problem: Setting the parameter to true\n1.17. TROUBLESHOOTING WITH THE MUST-GATHER COMMAND\n1.17.1. Symptom: Errors with multicluster global hub\n1.17.2. Resolving the problem: Running the must-gather command for dubugging\n1.17.2.1. Prerequisites\n1.17.2.2. Running the must-gather command\n1.18. TROUBLESHOOTING BY ACCESSING THE POSTGRESQL DATABASE\n1.18.1. Symptom: Errors with multicluster global hub\n1.18.2. Resolving the problem: Accessing the PostgresSQL database\n1.19.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3555, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8627330-4131-45ca-804c-4d3bd1567b74": {"__data__": {"id_": "e8627330-4131-45ca-804c-4d3bd1567b74", "embedding": null, "metadata": {"page_label": "6", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de721a4-671c-4833-b793-889701330075", "node_type": "4", "metadata": {"page_label": "6", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "a0775d45fb8e13f8bf01ad5c753676c12f37428d8144cf6fc3c8b4fe5b592f60", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01244469-7df9-4bce-9dbc-d53a0a3b7a09", "node_type": "1", "metadata": {"page_label": "6", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "3702151f8cb1fb0bdd21e5ffc6aba9f4d6a28fd56300363bbba63815012e4b12", "class_name": "RelatedNodeInfo"}}, "text": "Symptom: Volumesnapshot error state\n1.16.2. Resolving the problem: Setting the parameter to true\n1.17. TROUBLESHOOTING WITH THE MUST-GATHER COMMAND\n1.17.1. Symptom: Errors with multicluster global hub\n1.17.2. Resolving the problem: Running the must-gather command for dubugging\n1.17.2.1. Prerequisites\n1.17.2.2. Running the must-gather command\n1.18. TROUBLESHOOTING BY ACCESSING THE POSTGRESQL DATABASE\n1.18.1. Symptom: Errors with multicluster global hub\n1.18.2. Resolving the problem: Accessing the PostgresSQL database\n1.19. TROUBLESHOOTING BY USING THE DATABASE DUMP AND RESTORE\n18\n18\n18\n18\n18\n18\n18\n18\n18\n19\n19\n19\n19\n19\n19\n19\n19\n20\n20\n20\n20\n21\n21\n21\n21\n22\n22\n22\n23\n24\n24\n24\n24\n24\n25\n25\n25\n25\n25\n26\n26\n26\n26\n27\n27\n27\n28\nRed Hat Advanced Cluster Management for Kubernetes 2.11 Troubleshooting\n2", "mimetype": "text/plain", "start_char_idx": 3028, "end_char_idx": 3825, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8ab4073-7432-4091-9796-017b329c6ed6": {"__data__": {"id_": "a8ab4073-7432-4091-9796-017b329c6ed6", "embedding": null, "metadata": {"page_label": "7", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "378dbcc3-837f-465f-8e96-c5732b85de38", "node_type": "4", "metadata": {"page_label": "7", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "b870057c2bc2066fa622bb48dcdb8d23c00de57786102f85aba851c32ef3d091", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b38aa4a-304b-4c42-94a7-11d989962f66", "node_type": "1", "metadata": {}, "hash": "4a3b5380057790b02981ce5749a1715cf83a2ee9e5a547cf81f7d937b3c07e07", "class_name": "RelatedNodeInfo"}}, "text": "1.19.1. Symptom: Errors with multicluster global hub\n1.19.2. Resolving the problem: Dumping the output of the database for dubugging\n1.19.3. Resolving the problem: Restore database from dump\n1.20. TROUBLESHOOTING CLUSTER STATUS CHANGING FROM OFFLINE TO AVAILABLE\n1.20.1. Symptom: Cluster status changing from offline to available\n1.20.2. Resolving the problem: Cluster status changing from offline to available\n1.21. TROUBLESHOOTING CLUSTER IN CONSOLE WITH PENDING OR FAILED STATUS\n1.21.1. Symptom: Cluster in console with pending or failed status\n1.21.2. Identifying the problem: Cluster in console with pending or failed status\n1.21.3. Resolving the problem: Cluster in console with pending or failed status\n1.22. TROUBLESHOOTING GRAFANA\n1.22.1. Symptom: Grafana explorer gateway timeout\n1.22.2. Resolving the problem: Configure the Grafana\n1.23. TROUBLESHOOTING LOCAL CLUSTER NOT SELECTED WITH PLACEMENT RULE\n1.23.1. Symptom: Troubleshooting local cluster not selected as a managed cluster\n1.23.2. Resolving the problem: Troubleshooting local cluster not selected as a managed cluster\n1.24. TROUBLESHOOTING APPLICATION KUBERNETES DEPLOYMENT VERSION\n1.24.1. Symptom: Application deployment version\n1.24.2. Resolving the problem: Application deployment version\n1.25. TROUBLESHOOTING KLUSTERLET WITH DEGRADED CONDITIONS\n1.25.1. Symptom: Klusterlet is in the degraded condition\n1.25.2. Identifying the problem: Klusterlet is in the degraded condition\n1.25.3. Resolving the problem: Klusterlet is in the degraded condition\n1.26. TROUBLESHOOTING OBJECT STORAGE CHANNEL SECRET\n1.26.1. Symptom: Object storage channel secret\n1.26.2. Resolving the problem: Object storage channel secret\n1.27. TROUBLESHOOTING OBSERVABILITY\n1.27.1. Symptom: MultiClusterObservability resource status stuck\n1.27.2. Resolving the problem: MultiClusterObservability resource status stuck\n1.28. TROUBLESHOOTING OPENSHIFT MONITORING SERVICE\n1.28.1. Symptom: OpenShift monitoring service is not ready\n1.28.2. Resolving the problem: OpenShift monitoring service is not ready\n1.29. TROUBLESHOOTING METRICS-COLLECTOR\n1.29.1. Symptom: metrics-collector cannot verify observability-client-ca-certificate\n1.29.2. Resolving the problem: metrics-collector cannot verify observability-client-ca-certificate\n1.30. TROUBLESHOOTING POSTGRESQL SHARED MEMORY ERROR\n1.30.1. Symptom: PostgreSQL shared memory error\n1.30.2. Resolving the problem: PostgreSQL shared memory error\n1.31. TROUBLESHOOTING SUBMARINER NOT CONNECTING AFTER INSTALLATION\n1.31.1. Symptom: Submariner not connecting after installation\n1.31.2. Identifying the problem: Submariner not connecting after installation\n1.31.3. Resolving the problem: Submariner not connecting after installation\n1.32. TROUBLESHOOTING SUBMARINER ADD-ON STATUS IS DEGRADED\n1.32.1. Symptom: Submariner add-on status is degraded\n1.32.2. Resolving the problem: Submariner add-on status is degraded\n1.33. TROUBLESHOOTING RESTORE STATUS FINISHES WITH ERRORS\n1.33.1. Symptom: Troubleshooting restore status finishes with errors\n1.33.2. Resolving the problem: Troubleshooting restore status finishes with errors\n1.34. TROUBLESHOOTING MULTILINE YAML PARSING\n1.34.1. Symptom: Troubleshooting multiline YAML parsing\n1.34.2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3213, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b38aa4a-304b-4c42-94a7-11d989962f66": {"__data__": {"id_": "2b38aa4a-304b-4c42-94a7-11d989962f66", "embedding": null, "metadata": {"page_label": "7", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "378dbcc3-837f-465f-8e96-c5732b85de38", "node_type": "4", "metadata": {"page_label": "7", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "b870057c2bc2066fa622bb48dcdb8d23c00de57786102f85aba851c32ef3d091", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8ab4073-7432-4091-9796-017b329c6ed6", "node_type": "1", "metadata": {"page_label": "7", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "5464eed4dfebb3420ffb9af04f2a7e8acec6105d08be415a458aa9d1db074b03", "class_name": "RelatedNodeInfo"}}, "text": "Identifying the problem: Submariner not connecting after installation\n1.31.3. Resolving the problem: Submariner not connecting after installation\n1.32. TROUBLESHOOTING SUBMARINER ADD-ON STATUS IS DEGRADED\n1.32.1. Symptom: Submariner add-on status is degraded\n1.32.2. Resolving the problem: Submariner add-on status is degraded\n1.33. TROUBLESHOOTING RESTORE STATUS FINISHES WITH ERRORS\n1.33.1. Symptom: Troubleshooting restore status finishes with errors\n1.33.2. Resolving the problem: Troubleshooting restore status finishes with errors\n1.34. TROUBLESHOOTING MULTILINE YAML PARSING\n1.34.1. Symptom: Troubleshooting multiline YAML parsing\n1.34.2. Resolving the problem: Troubleshooting multiline YAML parsing\n28\n28\n29\n29\n29\n29\n30\n30\n30\n31\n31\n31\n31\n32\n32\n32\n33\n34\n34\n34\n34\n34\n35\n35\n35\n35\n36\n36\n36\n37\n37\n37\n37\n38\n38\n38\n38\n38\n39\n39\n39\n39\n40\n40\n41\n42\n42\n42\n42\n42\n43\nTable of Contents\n3", "mimetype": "text/plain", "start_char_idx": 2568, "end_char_idx": 3448, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25148466-216d-4c24-b5e3-44153e8a90d7": {"__data__": {"id_": "25148466-216d-4c24-b5e3-44153e8a90d7", "embedding": null, "metadata": {"page_label": "8", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d39d55fe-ee32-4125-91d1-a48c844bf232", "node_type": "4", "metadata": {"page_label": "8", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "00c2bf588c773a4a346bf3a5e2e8171f89d23f61a25bbbdd16e0d11f739ee5b5", "class_name": "RelatedNodeInfo"}}, "text": "Red Hat Advanced Cluster Management for Kubernetes 2.11 Troubleshooting\n4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 73, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1e03637-5d3a-4e7d-bd97-5b0ccf50e1bb": {"__data__": {"id_": "e1e03637-5d3a-4e7d-bd97-5b0ccf50e1bb", "embedding": null, "metadata": {"page_label": "9", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fc2fdffb-6777-43f9-b5fd-4206697c876a", "node_type": "4", "metadata": {"page_label": "9", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "c297c5f2b61fe47922e9a8987dc0f57aa337cea2b2718427b49efa1fd87ad003", "class_name": "RelatedNodeInfo"}}, "text": "CHAPTER 1. TROUBLESHOOTING\nBefore using the Troubleshooting guide, you can run the \noc adm must-gather\n command to gather\ndetails, logs, and take steps in debugging issues. For more details, see \nRunning the must-gather\ncommand to troubleshoot\n.\nAdditionally, check your role-based access. See \nRole-based access control\n for details.\n1.1. DOCUMENTED TROUBLESHOOTING\nView the list of troubleshooting topics for Red Hat Advanced Cluster Management for Kubernetes:\nInstallation\nTo view the main documentation for the installing tasks, see \nInstalling and upgrading\n.\nTroubleshooting installation status stuck in installing or pending\nTroubleshooting reinstallation failure\nTroubleshooting ocm-controller errors after Red Hat Advanced Cluster Management upgrade\nBackup and restore\nTo view the main documentation for backup and restore, see \nBackup and restore\n.\nTroubleshooting restore status finishes with errors\nCluster management\nTo view the main documentation about managing your clusters, see \nThe multicluster engine operator\ncluster lifecycle overview\n.\nTroubleshooting an offline cluster\nTroubleshooting a managed cluster import failure\nTroubleshooting cluster with pending import status\nTroubleshooting imported clusters offline after certificate change\nTroubleshooting cluster status changing from offline to available\nTroubleshooting cluster creation on VMware vSphere\nTroubleshooting cluster in console with pending or failed status\nTroubleshooting OpenShift Container Platform version 3.11 cluster import failure\nTroubleshooting Klusterlet with degraded conditions\nTroubleshooting Object storage channel secret\nNamespace remains after deleting a cluster\nAuto-import-secret-exists error when importing a cluster\nCHAPTER 1. TROUBLESHOOTING\n5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1749, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b4b2e8d-520c-4b74-85e1-278d1f44c84e": {"__data__": {"id_": "2b4b2e8d-520c-4b74-85e1-278d1f44c84e", "embedding": null, "metadata": {"page_label": "10", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ee83bd6-5651-4808-91b6-3690152713da", "node_type": "4", "metadata": {"page_label": "10", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "3a796c13924d927387f788a104005ba5ea74ae198c754d97140a580c1cfa69a5", "class_name": "RelatedNodeInfo"}}, "text": "Troubleshooting the cinder Container Storage Interface (CSI) driver for VolSync\nmulticluster global hub\nTo view the main documentation about the multicluster global hub, see \nmulticluster global hub\n.\nTroubleshooting with the \nmust-gather\n command\nTroubleshooting by accessing the PostgreSQL database\nTroubleshooting by using the database dump and restore\nApplication management\nTo view the main documentation about application management, see \nManaging applications\n.\nTroubleshooting application Kubernetes deployment version\nTroubleshooting local cluster not selected\nGovernance\nTroubleshooting multiline YAML parsing\nTo view the security guide, see \nRisk and compliance\n.\nConsole observability\nConsole observability includes Search, along with header and navigation function. To view the\nobservability guide, see \nObservability in the console\n.\nTroubleshooting grafana\nTroubleshooting observability\nTroubleshooting OpenShift monitoring services\nTroubleshooting metrics-collector\nTroubleshooting PostgreSQL shared memory error\nSubmariner networking and service discovery\nThis section lists the Submariner troubleshooting procedures that can occur when using Submariner with\nRed Hat Advanced Cluster Management or multicluster engine operator. For general Submariner\ntroubleshooting information, see \nTroubleshooting\n in the Submariner documentation.\nTo view the main documentation for the Submariner networking service and service discovery, see\nSubmariner multicluster networking and service discovery\n.\nTroubleshooting Submariner not connecting after installation - general information\nTroubleshooting Submariner add-on status is degraded\n1.2. RUNNING THE MUST-GATHER COMMAND TO TROUBLESHOOT\nTo get started with troubleshooting, learn about the troubleshooting scenarios for users to run the \nRed Hat Advanced Cluster Management for Kubernetes 2.11 Troubleshooting\n6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1870, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d240cfa0-7084-4ea0-b329-b6dd7ad61798": {"__data__": {"id_": "d240cfa0-7084-4ea0-b329-b6dd7ad61798", "embedding": null, "metadata": {"page_label": "11", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fe93d5e1-9887-4c49-a202-0e99dae0ecb6", "node_type": "4", "metadata": {"page_label": "11", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "f416c6a85cb7b392d6649659e2fb5a8e69a5c94778f6e4cf4a2a48cf5ca1ed3c", "class_name": "RelatedNodeInfo"}}, "text": "To get started with troubleshooting, learn about the troubleshooting scenarios for users to run the \nmust-gather\n command to debug the issues, then see the procedures to start using the command.\nRequired access:\n Cluster administrator\n1.2.1. Must-gather scenarios\nScenario one:\n Use the \nDocumented troubleshooting\n section to see if a solution to your problem\nis documented. The guide is organized by the major functions of the product.\nWith this scenario, you check the guide to see if your solution is in the documentation. For\ninstance, for trouble with creating a cluster, you might find a solution in the \nManage cluster\nsection.\nScenario two:\n If your problem is not documented with steps to resolve, run the \nmust-gather\ncommand and use the output to debug the issue.\nScenario three:\n If you cannot debug the issue using your output from the \nmust-gather\ncommand, then share your output with Red Hat Support.\n1.2.2. Must-gather procedure\nSee the following procedure to start using the \nmust-gather\n command:\n1\n. \nLearn about the \nmust-gather\n command and install the prerequisites that you need at\nGathering data about your cluster\n in the Red Hat OpenShift Container Platform\ndocumentation.\n2\n. \nLog in to your cluster. Add the Red Hat Advanced Cluster Management for Kubernetes image\nthat is used for gathering data and the directory. Run the following command, where you insert\nthe image and the directory for the output:\noc adm must-gather --image=registry.redhat.io/rhacm2/acm-must-gather-rhel9:v2.11 --dest-\ndir=<directory>\n3\n. \nFor the usual use-case, you should run the \nmust-gather\n while you are logged into your \nhub\ncluster.\nNote:\n If you want to check your managed clusters, find the \ngather-managed.log\n file that is\nlocated in the \ncluster-scoped-resources\n directory:\n<your-directory>/cluster-scoped-resources/gather-managed.log>\nCheck for managed clusters that are not set \nTrue\n for the JOINED and AVAILABLE column. You\ncan run the \nmust-gather\n command on those clusters that are not connected with \nTrue\n status.\n4\n. \nGo to your specified directory to see your output, which is organized in the following levels:\nTwo peer levels: \ncluster-scoped-resources\n and \nnamespace\n resources.\nSub-level for each: API group for the custom resource definitions for both cluster-scope\nand namespace-scoped resources.\nNext level for each: YAML file sorted by \nkind\n.\n1.2.3. Must-gather in a disconnected environment\nCHAPTER 1. TROUBLESHOOTING\n7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2459, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "167ac1ab-ca20-4bbd-812c-e3e47dc83686": {"__data__": {"id_": "167ac1ab-ca20-4bbd-812c-e3e47dc83686", "embedding": null, "metadata": {"page_label": "12", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25035d50-503f-4019-92e5-8d4244d9fc31", "node_type": "4", "metadata": {"page_label": "12", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "00dc6b9c77ffdfcb1f41fe02469e2c46c775faf6d6ce80e9f23d95fad97fb84b", "class_name": "RelatedNodeInfo"}}, "text": "Complete the following steps to run the \nmust-gather\n command in a disconnected environment:\n1\n. \nIn a disconnected environment, mirror the Red Hat operator catalog images into their mirror\nregistry. For more information, see \nInstall in disconnected network environments\n.\n2\n. \nRun the following commands to collect all of the information, replacing \n<2.x>\n with the\nsupported version for both \n<acm-must-gather>\n, for example \n2.10\n, and \n<multicluster-\nengine/must-gather>\n, for example \n2.5\n.\nREGISTRY=<internal.repo.address:port>\nIMAGE1=$REGISTRY/rhacm2/acm-must-gather-rhel9:v<2.x>\noc adm must-gather --image=$IMAGE1 --dest-dir=<directory>\nIf you experience issues with one of the currently supported releases, or the product documentation, go\nto \nRed Hat Support\n where you can troubleshoot further, view Knowledgebase articles, connect with the\nSupport Team, or open a case. You must log in with your Red Hat credentials.\n1.2.4. Must-gather for a hosted cluster\nIf you experience issues with hosted control plane clusters, you can run the \nmust-gather\n command to\ngather information to help you with troubleshooting.\n1.2.4.1. About the must-gather command for hosted clusters\nThe command generates output for the managed cluster and the hosted cluster.\nData from the multicluster engine operator hub cluster:\nCluster-scoped resources: These resources are node definitions of the management\ncluster.\nThe \nhypershift-dump\n compressed file: This file is useful if you need to share the content\nwith other people.\nNamespaced resources: These resources include all of the objects from the relevant\nnamespaces, such as config maps, services, events, and logs.\nNetwork logs: These logs include the OVN northbound and southbound databases and the\nstatus for each one.\nHosted clusters: This level of output involves all of the resources inside of the hosted\ncluster.\nData from the hosted cluster:\nCluster-scoped resources: These resources include all of the cluster-wide objects, such as\nnodes and CRDs.\nNamespaced resources: These resources include all of the objects from the relevant\nnamespaces, such as config maps, services, events, and logs.\nAlthough the output does not contain any secret objects from the cluster, it can contain references to\nthe names of the secrets.\n1.2.4.2. Prerequisites\nRed Hat Advanced Cluster Management for Kubernetes 2.11 Troubleshooting\n8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2372, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd7454dd-baec-4aef-a45e-c617c6bf8069": {"__data__": {"id_": "cd7454dd-baec-4aef-a45e-c617c6bf8069", "embedding": null, "metadata": {"page_label": "13", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3fe02681-cd77-44dc-9cd3-c51406f0c13b", "node_type": "4", "metadata": {"page_label": "13", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "ccf5531fcebad4f70186851ef3a6aa6c9dec0a6730e3fd55d751bc48062a4b1e", "class_name": "RelatedNodeInfo"}}, "text": "To gather information by running the must-gather command, you must meet the following prerequisites:\nYou must ensure that the \nkubeconfig\n file is loaded and is pointing to the multicluster engine\noperator hub cluster.\nYou must have cluster-admin access to the multicluster engine operator hub cluster.\nYou must have the name value for the \nHostedCluster\n resource and the namespace where the\ncustom resource is deployed.\n1.2.4.3. Entering the must-gather command for hosted clusters\n1\n. \nEnter the following command to collect information about the hosted cluster. In the command,\nthe \nhosted-cluster-namespace=HOSTEDCLUSTERNAMESPACE\n parameter is optional; if\nyou do not include it, the command runs as though the hosted cluster is in the default\nnamespace, which is \nclusters\n.\noc adm must-gather --image=quay.io/stolostron/backplane-must-gather:SNAPSHOTNAME\n \n/usr/bin/gather hosted-cluster-namespace=HOSTEDCLUSTERNAMESPACE hosted-cluster-\nname=HOSTEDCLUSTERNAME\n2\n. \nTo save the results of the command to a compressed file, include the \n--dest-dir=NAME\nparameter, replacing \nNAME\n with the name of the directory where you want to save the results:\noc adm must-gather --image=quay.io/stolostron/backplane-must-gather:SNAPSHOTNAME\n \n/usr/bin/gather hosted-cluster-namespace=HOSTEDCLUSTERNAMESPACE hosted-cluster-\nname=HOSTEDCLUSTERNAME --dest-dir=NAME ; tar -cvzf NAME.tgz NAME\n1.2.4.4. Entering the must-gather command in a disconnected environment\nComplete the following steps to run the \nmust-gather\n command in a disconnected environment:\n1\n. \nIn a disconnected environment, mirror the Red Hat operator catalog images into their mirror\nregistry. For more information, see \nInstall in disconnected network environments\n.\n2\n. \nRun the following command to extract logs, which reference the image from their mirror\nregistry:\nREGISTRY=registry.example.com:5000\nIMAGE=$REGISTRY/multicluster-engine/must-gather-\nrhel8@sha256:ff9f37eb400dc1f7d07a9b6f2da9064992934b69847d17f59e385783c071b9d8\noc adm must-gather --image=$IMAGE /usr/bin/gather hosted-cluster-\nnamespace=HOSTEDCLUSTERNAMESPACE hosted-cluster-\nname=HOSTEDCLUSTERNAME --dest-dir=./data\n1.2.4.5. Additional resources\nFor more information about troubleshooting hosted control planes, see \nTroubleshooting hosted\ncontrol planes\n in the OpenShift Container Platform documentation.\n1.3. TROUBLESHOOTING INSTALLATION STATUS STUCK IN\nINSTALLING OR PENDING\nCHAPTER 1. TROUBLESHOOTING\n9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2438, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cdb53133-cc84-4253-9043-70dcd60c421d": {"__data__": {"id_": "cdb53133-cc84-4253-9043-70dcd60c421d", "embedding": null, "metadata": {"page_label": "14", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9ac7f74d-d144-4bfe-b82d-a58c76ef80eb", "node_type": "4", "metadata": {"page_label": "14", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "06b520fff85cea612d5729fd47607eaf9cd794dff275720e44c104b9c6cf3e4b", "class_name": "RelatedNodeInfo"}}, "text": "When installing Red Hat Advanced Cluster Management, the \nMultiClusterHub\n remains in \nInstalling\nphase, or multiple pods maintain a \nPending\n status.\n1.3.1. Symptom: Stuck in Pending status\nMore than ten minutes passed since you installed \nMultiClusterHub\n and one or more components from\nthe \nstatus.components\n field of the \nMultiClusterHub\n resource report \nProgressDeadlineExceeded\n.\nResource constraints on the cluster might be the issue.\nCheck the pods in the namespace where \nMulticlusterhub\n was installed. You might see \nPending\n with a\nstatus similar to the following:\nreason: Unschedulable\nmessage: '0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had taint {node-\nrole.kubernetes.io/master:\n        }, that the pod didn't tolerate.'\nIn this case, the worker nodes resources are not sufficient in the cluster to run the product.\n1.3.2. Resolving the problem: Adjust worker node sizing\nIf you have this problem, then your cluster needs to be updated with either larger or more worker nodes.\nSee \nSizing your cluster\n for guidelines on sizing your cluster.\n1.4. TROUBLESHOOTING REINSTALLATION FAILURE\nWhen reinstalling Red Hat Advanced Cluster Management for Kubernetes, the pods do not start.\n1.4.1. Symptom: Reinstallation failure\nIf your pods do not start after you install Red Hat Advanced Cluster Management, it is likely that Red\nHat Advanced Cluster Management was previously installed, and not all of the pieces were removed\nbefore you attempted this installation.\nIn this case, the pods do not start after completing the installation process.\n1.4.2. Resolving the problem: Reinstallation failure\nIf you have this problem, complete the following steps:\n1\n. \nRun the uninstallation process to remove the current components by following the steps in\nUninstalling\n.\n2\n. \nInstall the Helm CLI binary version 3.2.0, or later, by following the instructions at \nInstalling Helm\n.\n3\n. \nEnsure that your Red Hat OpenShift Container Platform CLI is configured to run \noc\n commands.\nSee \nGetting started with the OpenShift CLI\n in the OpenShift Container Platform\ndocumentation for more information about how to configure the \noc\n commands.\n4\n. \nCopy the following script into a file:\n#!/bin/bash\nACM_NAMESPACE=<namespace>\noc delete mch --all -n $ACM_NAMESPACE\nRed Hat Advanced Cluster Management for Kubernetes 2.11 Troubleshooting\n10", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2349, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e752e4e-7153-48fe-88e7-73240050b054": {"__data__": {"id_": "2e752e4e-7153-48fe-88e7-73240050b054", "embedding": null, "metadata": {"page_label": "15", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "79336ac5-934d-435c-8a16-69634e5fc244", "node_type": "4", "metadata": {"page_label": "15", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "50fc32ab06471d8a851e3708da8681a67b1641c8500c1ac03804170c0986cf43", "class_name": "RelatedNodeInfo"}}, "text": "oc delete apiservice v1.admission.cluster.open-cluster-management.io\n \nv1.admission.work.open-cluster-management.io\noc delete clusterimageset --all\noc delete clusterrole multiclusterengines.multicluster.openshift.io-v1-admin\n \nmulticlusterengines.multicluster.openshift.io-v1-crdview\n \nmulticlusterengines.multicluster.openshift.io-v1-edit\n \nmulticlusterengines.multicluster.openshift.io-v1-view open-cluster-\nmanagement:addons:application-manager open-cluster-management:admin-aggregate open-\ncluster-management:cert-policy-controller-hub open-cluster-management:cluster-manager-\nadmin-aggregate open-cluster-management:config-policy-controller-hub open-cluster-\nmanagement:edit-aggregate open-cluster-management:iam-policy-controller-hub open-\ncluster-management:policy-framework-hub open-cluster-management:view-aggregate\noc delete crd klusterletaddonconfigs.agent.open-cluster-management.io\n \nplacementbindings.policy.open-cluster-management.io policies.policy.open-cluster-\nmanagement.io userpreferences.console.open-cluster-management.io\n \ndiscoveredclusters.discovery.open-cluster-management.io discoveryconfigs.discovery.open-\ncluster-management.io\noc delete mutatingwebhookconfiguration ocm-mutating-webhook\n \nmanagedclustermutators.admission.cluster.open-cluster-management.io multicluster-\nobservability-operator\noc delete validatingwebhookconfiguration\n \nchannels.apps.open.cluster.management.webhook.validator application-webhook-validator\n \nmulticlusterhub-operator-validating-webhook ocm-validating-webhook multicluster-\nobservability-operator multiclusterengines.multicluster.openshift.io\nReplace \n<namespace>\n in the script with the name of the namespace where Red Hat Advanced\nCluster Management was installed. Ensure that you specify the correct namespace, as the\nnamespace is cleaned out and deleted.\n5\n. \nRun the script to remove the artifacts from the previous installation.\n6\n. \nRun the installation. See \nInstalling while connected online\n.\n1.5. TROUBLESHOOTING OCM-CONTROLLER ERRORS AFTER RED\nHAT ADVANCED CLUSTER MANAGEMENT UPGRADE\nAfter you upgrade from 2.7.x to 2.8.x and then to 2.9.0, the \nocm-controller\n of the \nmulticluster-engine\nnamespace crashes.\n1.5.1. Symptom: Troubleshooting ocm-controller errors after Red Hat Advanced\nCluster Management upgrade\nAfter you attempt to list \nManagedClusterSet\n and \nManagedClusterSetBinding\n custom resource\ndefinitions, the following error message appears:\nThe previous message indicates that the migration of \nManagedClusterSets\n and \nManagedClusterSetBindings\n custom resource definitions from \nv1beta1\n to \nv1beta2\n failed.\n1.5.2. Resolving the problem: Troubleshooting ocm-controller errors after Red Hat\nAdvanced Cluster Management upgrade\nError from server: request to convert CR from an invalid group/version: cluster.open-cluster-\nmanagement.io/v1beta1\nCHAPTER 1. TROUBLESHOOTING\n11", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2863, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c761ea2-ac52-4827-b20f-83b8ebc1ade4": {"__data__": {"id_": "5c761ea2-ac52-4827-b20f-83b8ebc1ade4", "embedding": null, "metadata": {"page_label": "16", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7576b933-dde7-4076-a724-43dbad0db089", "node_type": "4", "metadata": {"page_label": "16", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "b3d0b125fb6f5bbbca65cd9c62ccaf4aa7ca2eef034d4bc3cf2ce1d0cb7f3fba", "class_name": "RelatedNodeInfo"}}, "text": "To resolve this error, you must initiate the API migration manually. Completet the following steps:\n1\n. \nRevert the \ncluster-manager\n to a previous release.\na\n. \nPause the \nmulticlusterengine\n with the following command:\nb\n. \nRun the following commands to replace the image of the \ncluster-manager\n deployment\nwith a previous version:\nc\n. \nReplace the registration image reference in the \nClusterManager\n resource with a previous\nversion. Run the following command:\n2\n. \nRun the following commands to revert the \nManagedClusterSets\n and \nManagedClusterSetBindings\n custom resource definitions to a previous release:\n3\n. \nRestart the \ncluster-manager\n and wait for the custom resource definitions to be recreated. Run\nthe following commands:\n4\n. \nStart the storage version migration with the following commands:\noc annotate mce multiclusterengine pause=\ntrue\noc patch deployment cluster-manager -n multicluster-engine -p \\  \n'{\"spec\":{\"template\":\n{\"spec\":{\"containers\":[{\"name\":\"registration-\noperator\",\"image\":\"registry.redhat.io/multicluster-engine/registration-operator-\nrhel8@sha256:35999c3a1022d908b6fe30aa9b85878e666392dbbd685e9f3edcb83e3336d\n19f\"}]}}}}'\nexport\n ORIGIN_REGISTRATION_IMAGE=$(oc get clustermanager cluster-manager -o\n \njsonpath=\n'{.spec.registrationImagePullSpec}'\n)\noc patch clustermanager cluster-manager --type=\n'json'\n -p=\n'[{\"op\": \"replace\", \"path\":\n \n\"/spec/registrationImagePullSpec\", \"value\": \"registry.redhat.io/multicluster-\nengine/registration-\nrhel8@sha256:a3c22aa4326859d75986bf24322068f0aff2103cccc06e1001faaf79b939051\n5\"}]'\noc annotate crds managedclustersets.cluster.open-cluster-management.io operator.open-\ncluster-management.io/version-\noc annotate crds  managedclustersetbindings.cluster.open-cluster-management.io\n \noperator.open-cluster-management.io/version-\noc -n multicluster-engine delete pods \n-l\n app=cluster-manager\noc \nwait\n crds managedclustersets.cluster.open-cluster-management.io --for=jsonpath=\n\"\n{.metadata.annotations['operator\\.open-cluster-management\\.io/version']}\"\n=\n\"2.3.3\"\n --\ntimeout=120s\noc \nwait\n crds managedclustersetbindings.cluster.open-cluster-management.io --\nfor=jsonpath=\n\"{.metadata.annotations['operator\\.open-cluster-\nmanagement\\.io/version']}\"\n=\n\"2.3.3\"\n --timeout=120s\noc patch StorageVersionMigration managedclustersets.cluster.open-cluster-management.io -\n-type=\n'json'\n -p=\n'[{\"op\":\"replace\", \"path\":\"/spec/resource/version\", \"value\":\"v1beta1\"}]'\noc patch StorageVersionMigration managedclustersets.cluster.open-cluster-management.io -\n-type=\n'json'\n --subresource status -p=\n'[{\"op\":\"remove\", \"path\":\"/status/conditions\"}]'\noc patch StorageVersionMigration managedclustersetbindings.cluster.open-cluster-\nRed Hat Advanced Cluster Management for Kubernetes 2.11 Troubleshooting\n12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2762, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "270ebb66-2cda-4460-8431-67c954bb91ab": {"__data__": {"id_": "270ebb66-2cda-4460-8431-67c954bb91ab", "embedding": null, "metadata": {"page_label": "17", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "05fecf46-7952-4cf4-9978-2e61face2f01", "node_type": "4", "metadata": {"page_label": "17", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "cf339199ffc7a9cecaf7bfc28a184266dae6a586a3a67a68acec84c1409641a6", "class_name": "RelatedNodeInfo"}}, "text": "5\n. \nRun the following command to wait for the migration to complete:\n6\n. \nRestore the \ncluster-manager\n back to Red Hat Advanced Cluster Management 2.11. It might\ntake several minutes. Run the following command:\n1.5.2.1. Verification\nTo verify that Red Hat Advanced Cluster Management is recovered run the following commands:\nAfter running the commands, the \nManagedClusterSets\n and \nManagedClusterSetBindings\n resources\nare listed without error messages.\n1.6. TROUBLESHOOTING AN OFFLINE CLUSTER\nThere are a few common causes for a cluster showing an offline status.\n1.6.1. Symptom: Cluster status is offline\nAfter you complete the procedure for creating a cluster, you cannot access it from the Red Hat\nAdvanced Cluster Management console, and it shows a status of \noffline\n.\n1.6.2. Resolving the problem: Cluster status is offline\n1\n. \nDetermine if the managed cluster is available. You can check this in the \nClusters\n area of the Red\nHat Advanced Cluster Management console.\nIf it is not available, try restarting the managed cluster.\n2\n. \nIf the managed cluster status is still offline, complete the following steps:\na\n. \nRun the \noc get managedcluster <cluster_name> -o yaml\n command on the hub cluster.\nReplace \n<cluster_name>\n with the name of your cluster.\nb\n. \nFind the \nstatus.conditions\n section.\nc\n. \nCheck the messages for \ntype: ManagedClusterConditionAvailable\n and resolve any\nmanagement.io --type=\n'json'\n -p=\n'[{\"op\":\"replace\", \"path\":\"/spec/resource/version\",\n \n\"value\":\"v1beta1\"}]'\noc patch StorageVersionMigration managedclustersetbindings.cluster.open-cluster-\nmanagement.io --type=\n'json'\n --subresource status -p=\n'[{\"op\":\"remove\",\n \n\"path\":\"/status/conditions\"}]'\noc \nwait\n storageversionmigration managedclustersets.cluster.open-cluster-management.io --\nfor=condition=Succeeded --timeout=120s\noc \nwait\n storageversionmigration managedclustersetbindings.cluster.open-cluster-\nmanagement.io --for=condition=Succeeded --timeout=120s\noc annotate mce multiclusterengine pause-\noc patch clustermanager cluster-manager --type=\n'json'\n -p=\n'[{\"op\": \"replace\", \"path\":\n \n\"/spec/registrationImagePullSpec\", \"value\": \"'\n$ORIGIN_REGISTRATION_IMAGE\n'\"}]'\noc get managedclusterset\noc get managedclustersetbinding -A\nCHAPTER 1. TROUBLESHOOTING\n13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2259, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0310e71-4439-45d2-8667-113f5691e793": {"__data__": {"id_": "b0310e71-4439-45d2-8667-113f5691e793", "embedding": null, "metadata": {"page_label": "18", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "15068f1a-bf98-477c-9f82-18f02d133f53", "node_type": "4", "metadata": {"page_label": "18", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "a25fa85d4a9a527de8e3f4318103b1bdbeed5d2916d7a675d8b752ccd62d8ee9", "class_name": "RelatedNodeInfo"}}, "text": "c\n. \nCheck the messages for \ntype: ManagedClusterConditionAvailable\n and resolve any\nproblems.\n1.7. TROUBLESHOOTING A MANAGED CLUSTER IMPORT FAILURE\nIf your cluster import fails, there are a few steps that you can take to determine why the cluster import\nfailed.\n1.7.1. Symptom: Imported cluster not available\nAfter you complete the procedure for importing a cluster, you cannot access it from the Red Hat\nAdvanced Cluster Management for Kubernetes console.\n1.7.2. Resolving the problem: Imported cluster not available\nThere can be a few reasons why an imported cluster is not available after an attempt to import it. If the\ncluster import fails, complete the following steps, until you find the reason for the failed import:\n1\n. \nOn the Red Hat Advanced Cluster Management hub cluster, run the following command to\nensure that the Red Hat Advanced Cluster Management import controller is running.\nkubectl -n multicluster-engine get pods -l app=managedcluster-import-controller-v2\nYou should see two pods that are running. If either of the pods is not running, run the following\ncommand to view the log to determine the reason:\nkubectl -n multicluster-engine logs -l app=managedcluster-import-controller-v2 --tail=-1\n2\n. \nOn the Red Hat Advanced Cluster Management hub cluster, run the following command to\ndetermine if the managed cluster import secret was generated successfully by the Red Hat\nAdvanced Cluster Management import controller:\nkubectl -n <managed_cluster_name> get secrets <managed_cluster_name>-import\nIf the import secret does not exist, run the following command to view the log entries for the\nimport controller and determine why it was not created:\nkubectl -n multicluster-engine logs -l app=managedcluster-import-controller-v2 --tail=-1 | grep\n \nimportconfig-controller\n3\n. \nOn the Red Hat Advanced Cluster Management hub cluster, if your managed cluster is \nlocal-\ncluster\n, provisioned by Hive, or has an auto-import secret, run the following command to check\nthe import status of the managed cluster.\nkubectl get managedcluster <managed_cluster_name> -o=jsonpath='{range\n \n.status.conditions[*]}{.type}{\"\\t\"}{.status}{\"\\t\"}{.message}{\"\\n\"}{end}' | grep\n \nManagedClusterImportSucceeded\nIf the condition \nManagedClusterImportSucceeded\n is not \ntrue\n, the result of the command\nindicates the reason for the failure.\n4\n. \nCheck the Klusterlet status of the managed cluster for a degraded condition. See\nRed Hat Advanced Cluster Management for Kubernetes 2.11 Troubleshooting\n14", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2498, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f41c79b-efbb-4768-b338-aae492905cd1": {"__data__": {"id_": "3f41c79b-efbb-4768-b338-aae492905cd1", "embedding": null, "metadata": {"page_label": "19", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38466272-90c3-4529-9605-f79180422b6b", "node_type": "4", "metadata": {"page_label": "19", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "bedcf11d18f955ad745b8826a0cbf3166823a087282818ddf4cd371d1c613ee6", "class_name": "RelatedNodeInfo"}}, "text": "4\n. \nCheck the Klusterlet status of the managed cluster for a degraded condition. See\nTroubleshooting Klusterlet with degraded conditions\n to find the reason that the Klusterlet is\ndegraded.\n1.8. TROUBLESHOOTING CLUSTER WITH PENDING IMPORT STATUS\nIf you receive \nPending import\n continually on the console of your cluster, follow the procedure to\ntroubleshoot the problem.\n1.8.1. Symptom: Cluster with pending import status\nAfter importing a cluster by using the Red Hat Advanced Cluster Management console, the cluster\nappears in the console with a status of \nPending import\n.\n1.8.2. Identifying the problem: Cluster with pending import status\n1\n. \nRun the following command on the managed cluster to view the Kubernetes pod names that are\nhaving the issue:\nkubectl get pod -n open-cluster-management-agent | grep klusterlet-registration-agent\n2\n. \nRun the following command on the managed cluster to find the log entry for the error:\nkubectl logs <registration_agent_pod> -n open-cluster-management-agent\nReplace \nregistration_agent_pod\n with the pod name that you identified in step 1.\n3\n. \nSearch the returned results for text that indicates there was a networking connectivity problem.\nExample includes: \nno such host\n.\n1.8.3. Resolving the problem: Cluster with pending import status\n1\n. \nRetrieve the port number that is having the problem by entering the following command on the\nhub cluster:\noc get infrastructure cluster -o yaml | grep apiServerURL\n2\n. \nEnsure that the hostname from the managed cluster can be resolved, and that outbound\nconnectivity to the host and port is occurring.\nIf the communication cannot be established by the managed cluster, the cluster import is not\ncomplete. The cluster status for the managed cluster is \nPending import\n.\n1.9. TROUBLESHOOTING CLUSTER WITH ALREADY EXISTS ERROR\nIf you are unable to import an OpenShift Container Platform cluster into Red Hat Advanced Cluster\nManagement \nMultiClusterHub\n and receive an \nAlreadyExists\n error, follow the procedure to\ntroubleshoot the problem.\n1.9.1. Symptom: Already exists error log when importing OpenShift Container\nPlatform cluster\nAn error log shows up when importing an OpenShift Container Platform cluster into Red Hat Advanced\nCHAPTER 1. TROUBLESHOOTING\n15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2255, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d9d072f3-56a7-47c1-a808-4a4ae658ea26": {"__data__": {"id_": "d9d072f3-56a7-47c1-a808-4a4ae658ea26", "embedding": null, "metadata": {"page_label": "20", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ee7fa24-2fe2-4836-8fba-62aab1276a4c", "node_type": "4", "metadata": {"page_label": "20", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "e3a9bac30186eb6219f845d6900196ac9ee1cc028891a98559a4fcc19ed1df4e", "class_name": "RelatedNodeInfo"}}, "text": "An error log shows up when importing an OpenShift Container Platform cluster into Red Hat Advanced\nCluster Management \nMultiClusterHub\n:\nerror log:\nWarning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+,\n \nunavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition\nError from server (AlreadyExists): error when creating \"STDIN\":\n \ncustomresourcedefinitions.apiextensions.k8s.io \"klusterlets.operator.open-cluster-management.io\"\n \nalready exists\nThe cluster cannot be imported because its Klusterlet CRD already exists.\nEither the cluster was already imported, or it was not detached completely during a previous detach\n \nprocess.\nDetach the existing cluster before trying the import again.\"\n1.9.2. Identifying the problem: Already exists when importing OpenShift Container\nPlatform cluster\nCheck if there are any Red Hat Advanced Cluster Management-related resources on the cluster that\nyou want to import to new the Red Hat Advanced Cluster Management \nMultiClusterHub\n by running the\nfollowing commands:\noc get all -n open-cluster-management-agent\noc get all -n open-cluster-management-agent-addon\n1.9.3. Resolving the problem: Already exists when importing OpenShift Container\nPlatform cluster\nRemove the \nklusterlet\n custom resource by using the following command:\nRun the following commands to remove pre-existing resources:\noc delete namespaces open-cluster-management-agent open-cluster-management-agent-addon --\nwait=false\noc get crds | grep open-cluster-management.io | awk '{print $1}' | xargs oc delete crds --wait=false\noc get crds | grep open-cluster-management.io | awk '{print $1}' | xargs oc patch crds --type=merge -\np '{\"metadata\":{\"finalizers\": []}}'\n1.10. TROUBLESHOOTING CLUSTER CREATION ON VMWARE\nVSPHERE\nIf you experience a problem when creating a Red Hat OpenShift Container Platform cluster on VMware\nvSphere, see the following troubleshooting information to see if one of them addresses your problem.\nNote:\n Sometimes when the cluster creation process fails on VMware vSphere, the link is not enabled for\nyou to view the logs. If this happens, you can identify the problem by viewing the log of the \nhive-\ncontrollers\n pod. The \nhive-controllers\n log is in the \nhive\n namespace.\n1.10.1. Managed cluster creation fails with certificate IP SAN error\noc get klusterlet | grep klusterlet | awk \n'{print $1}'\n | xargs oc patch klusterlet --type=merge -p\n \n'{\"metadata\":{\"finalizers\": []}}'\nRed Hat Advanced Cluster Management for Kubernetes 2.11 Troubleshooting\n16", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2534, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e7773a0-93c1-4d79-82f6-d29464867d6f": {"__data__": {"id_": "9e7773a0-93c1-4d79-82f6-d29464867d6f", "embedding": null, "metadata": {"page_label": "21", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d383f5fe-c49c-41b8-bd0c-9c611eaf3204", "node_type": "4", "metadata": {"page_label": "21", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "fbe51a640e6bee47e775222d8012362c848932fa1d7f1d66b1830cbaf1f3aa72", "class_name": "RelatedNodeInfo"}}, "text": "1.10.1.1. Symptom: Managed cluster creation fails with certificate IP SAN error\nAfter creating a new Red Hat OpenShift Container Platform cluster on VMware vSphere, the cluster fails\nwith an error message that indicates a certificate IP SAN error.\n1.10.1.2. Identifying the problem: Managed cluster creation fails with certificate IP SAN error\nThe deployment of the managed cluster fails and returns the following errors in the deployment log:\ntime=\"2020-08-07T15:27:55Z\" level=error msg=\"Error: error setting up new vSphere SOAP client:\n \nPost https://147.1.1.1/sdk: x509: cannot validate certificate for xx.xx.xx.xx because it doesn't contain\n \nany IP SANs\"\ntime=\"2020-08-07T15:27:55Z\" level=error\n1.10.1.3. Resolving the problem: Managed cluster creation fails with certificate IP SAN error\nUse the VMware vCenter server fully-qualified host name instead of the IP address in the credential.\nYou can also update the VMware vCenter CA certificate to contain the IP SAN.\n1.10.2. Managed cluster creation fails with unknown certificate authority\n1.10.2.1. Symptom: Managed cluster creation fails with unknown certificate authority\nAfter creating a new Red Hat OpenShift Container Platform cluster on VMware vSphere, the cluster fails\nbecause the certificate is signed by an unknown authority.\n1.10.2.2. Identifying the problem: Managed cluster creation fails with unknown certificate\nauthority\nThe deployment of the managed cluster fails and returns the following errors in the deployment log:\nError: error setting up new vSphere SOAP client: Post https://vspherehost.com/sdk: x509: certificate\n \nsigned by unknown authority\"\n1.10.2.3. Resolving the problem: Managed cluster creation fails with unknown certificate\nauthority\nEnsure you entered the correct certificate from the certificate authority when creating the credential.\n1.10.3. Managed cluster creation fails with expired certificate\n1.10.3.1. Symptom: Managed cluster creation fails with expired certificate\nAfter creating a new Red Hat OpenShift Container Platform cluster on VMware vSphere, the cluster fails\nbecause the certificate is expired or is not yet valid.\n1.10.3.2. Identifying the problem: Managed cluster creation fails with expired certificate\nThe deployment of the managed cluster fails and returns the following errors in the deployment log:\nx509: certificate has expired or is not yet valid\nCHAPTER 1. TROUBLESHOOTING\n17", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2397, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "79c2abe9-351c-4c80-990c-928442cd7fb2": {"__data__": {"id_": "79c2abe9-351c-4c80-990c-928442cd7fb2", "embedding": null, "metadata": {"page_label": "22", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bc98ad6f-e751-4ad6-9e92-65875c0be89f", "node_type": "4", "metadata": {"page_label": "22", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "942fcf27ef06f3765f5d0f39e097e4aa6fc32c524c78ab157074c156e50ad4bb", "class_name": "RelatedNodeInfo"}}, "text": "1.10.3.3. Resolving the problem: Managed cluster creation fails with expired certificate\nEnsure that the time on your ESXi hosts is synchronized.\n1.10.4. Managed cluster creation fails with insufficient privilege for tagging\n1.10.4.1. Symptom: Managed cluster creation fails with insufficient privilege for tagging\nAfter creating a new Red Hat OpenShift Container Platform cluster on VMware vSphere, the cluster fails\nbecause there is insufficient privilege to use tagging.\n1.10.4.2. Identifying the problem: Managed cluster creation fails with insufficient privilege\nfor tagging\nThe deployment of the managed cluster fails and returns the following errors in the deployment log:\ntime=\"2020-08-07T19:41:58Z\" level=debug msg=\"vsphere_tag_category.category: Creating...\"\ntime=\"2020-08-07T19:41:58Z\" level=error\ntime=\"2020-08-07T19:41:58Z\" level=error msg=\"Error: could not create category: POST\n \nhttps://vspherehost.com/rest/com/vmware/cis/tagging/category: 403 Forbidden\"\ntime=\"2020-08-07T19:41:58Z\" level=error\ntime=\"2020-08-07T19:41:58Z\" level=error msg=\"  on ../tmp/openshift-install-436877649/main.tf line\n \n54, in resource \\\"vsphere_tag_category\\\" \\\"category\\\":\"\ntime=\"2020-08-07T19:41:58Z\" level=error msg=\"  54: resource \\\"vsphere_tag_category\\\" \\\"category\\\"\n \n{\"\n1.10.4.3. Resolving the problem: Managed cluster creation fails with insufficient privilege for\ntagging\nEnsure that your VMware vCenter required account privileges are correct. See \nImage registry removed\nduring information\n for more information.\n1.10.5. Managed cluster creation fails with invalid dnsVIP\n1.10.5.1. Symptom: Managed cluster creation fails with invalid dnsVIP\nAfter creating a new Red Hat OpenShift Container Platform cluster on VMware vSphere, the cluster fails\nbecause there is an invalid dnsVIP.\n1.10.5.2. Identifying the problem: Managed cluster creation fails with invalid dnsVIP\nIf you see the following message when trying to deploy a new managed cluster with VMware vSphere, it\nis because you have an older OpenShift Container Platform release image that does not support\nVMware Installer Provisioned Infrastructure (IPI):\nfailed to fetch Master Machines: failed to load asset \\\\\\\"Install Config\\\\\\\": invalid \\\\\\\"install-\nconfig.yaml\\\\\\\" file: platform.vsphere.dnsVIP: Invalid value: \\\\\\\"\\\\\\\": \\\\\\\"\\\\\\\" is not a valid IP\n1.10.5.3. Resolving the problem: Managed cluster creation fails with invalid dnsVIP\nSelect a release image from a later version of OpenShift Container Platform that supports VMware\nInstaller Provisioned Infrastructure.\nRed Hat Advanced Cluster Management for Kubernetes 2.11 Troubleshooting\n18", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2609, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2495292d-46f8-4ab6-bf8c-1169511f9b7e": {"__data__": {"id_": "2495292d-46f8-4ab6-bf8c-1169511f9b7e", "embedding": null, "metadata": {"page_label": "23", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "08a3bfd8-a1d3-4791-aa06-4cce0c59db37", "node_type": "4", "metadata": {"page_label": "23", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "4facddaf8f6a959c0d07b225ab4107405aaaded17bd7d91def669f9fc4de3e7c", "class_name": "RelatedNodeInfo"}}, "text": "1.10.6. Managed cluster creation fails with incorrect network type\n1.10.6.1. Symptom: Managed cluster creation fails with incorrect network type\nAfter creating a new Red Hat OpenShift Container Platform cluster on VMware vSphere, the cluster fails\nbecause there is an incorrect network type specified.\n1.10.6.2. Identifying the problem: Managed cluster creation fails with incorrect network type\nIf you see the following message when trying to deploy a new managed cluster with VMware vSphere, it\nis because you have an older OpenShift Container Platform image that does not support VMware\nInstaller Provisioned Infrastructure (IPI):\ntime=\"2020-08-11T14:31:38-04:00\" level=debug msg=\"vsphereprivate_import_ova.import:\n \nCreating...\"\ntime=\"2020-08-11T14:31:39-04:00\" level=error\ntime=\"2020-08-11T14:31:39-04:00\" level=error msg=\"Error: rpc error: code = Unavailable desc =\n \ntransport is closing\"\ntime=\"2020-08-11T14:31:39-04:00\" level=error\ntime=\"2020-08-11T14:31:39-04:00\" level=error\ntime=\"2020-08-11T14:31:39-04:00\" level=fatal msg=\"failed to fetch Cluster: failed to generate asset\n \n\\\"Cluster\\\": failed to create cluster: failed to apply Terraform: failed to complete the change\"\n1.10.6.3. Resolving the problem: Managed cluster creation fails with incorrect network type\nSelect a valid VMware vSphere network type for the specified VMware cluster.\n1.10.7. Managed cluster creation fails with an error processing disk changes\n1.10.7.1. Symptom: Adding the VMware vSphere managed cluster fails due to an error\nprocessing disk changes\nAfter creating a new Red Hat OpenShift Container Platform cluster on VMware vSphere, the cluster fails\nbecause there is an error when processing disk changes.\n1.10.7.2. Identifying the problem: Adding the VMware vSphere managed cluster fails due to\nan error processing disk changes\nA message similar to the following is displayed in the logs:\nERROR\nERROR Error: error reconfiguring virtual machine: error processing disk changes post-clone: disk.0:\n \nServerFaultCode: NoPermission: RESOURCE (vm-71:2000), ACTION (queryAssociatedProfile):\n \nRESOURCE (vm-71), ACTION (PolicyIDByVirtualDisk)\n1.10.7.3. Resolving the problem: Adding the VMware vSphere managed cluster fails due to\nan error processing disk changes\nUse the VMware vSphere client to give the user \nAll privileges\n for \nProfile-driven Storage Privileges\n.\n1.11. MANAGED CLUSTER CREATION FAILS ON RED HAT OPENSTACK\nCHAPTER 1. TROUBLESHOOTING\n19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2440, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b8d2fa4-4b70-4b1e-ac25-6075d86f376a": {"__data__": {"id_": "0b8d2fa4-4b70-4b1e-ac25-6075d86f376a", "embedding": null, "metadata": {"page_label": "24", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "210f6cf2-7cad-4c99-af9c-b15241620d9e", "node_type": "4", "metadata": {"page_label": "24", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "6b828f568cdcc19281e9e59cc67c111b57f29c1f7a0bc6f4e3113058f604a9f1", "class_name": "RelatedNodeInfo"}}, "text": "1.11. MANAGED CLUSTER CREATION FAILS ON RED HAT OPENSTACK\nPLATFORM WITH UNKNOWN AUTHORITY ERROR\nIf you experience a problem when creating a Red Hat OpenShift Container Platform cluster on Red Hat\nOpenStack Platform, see the following troubleshooting information to see if one of them addresses your\nproblem.\n1.11.1. Symptom: Managed cluster creation fails with unknown authority error\nAfter creating a new Red Hat OpenShift Container Platform cluster on Red Hat OpenStack Platform\nusing self-signed certificates, the cluster fails with an error message that indicates an unknown authority\nerror.\n1.11.2. Identifying the problem: Managed cluster creation fails with unknown\nauthority error\nThe deployment of the managed cluster fails and returns the following error message:\nx509: certificate signed by unknown authority\n1.11.3. Resolving the problem: Managed cluster creation fails with unknown authority\nerror\nVerify that the following files are configured correctly:\n1\n. \nThe \nclouds.yaml\n file must specify the path to the \nca.crt\n file in the \ncacert\n parameter. The \ncacert\n parameter is passed to the OpenShift installer when generating the ignition shim. See\nthe following example:\n2\n. \nThe \ncertificatesSecretRef\n paremeter must reference a secret with a file name matching the \nca.crt\n file. See the following example:\nTo create a secret with a matching file name, run the following command:\noc create secret generic txue-osspoke-openstack-certificatebundle --from-\nfile=ca.crt=ca.crt.pem -n $CLUSTERNAME\n3\n. \nThe size of the \nca.cert\n file must be less than 63 thousand bytes.\nclouds:\n  openstack:\n    cacert:\n \n\"/etc/pki/ca-trust/source/anchors/ca.crt\"\nspec:\n  baseDomain:\n dev09.red-chesterfield.com\n  clusterName:\n txue-osspoke\n  platform:\n    openstack:\n      cloud:\n openstack\n      credentialsSecretRef:\n        name:\n txue-osspoke-openstack-creds\n      certificatesSecretRef:\n        name:\n txue-osspoke-openstack-certificatebundle\nRed Hat Advanced Cluster Management for Kubernetes 2.11 Troubleshooting\n20", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2023, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b7ee835-ddec-44f7-98c9-89c8ac94a2e6": {"__data__": {"id_": "2b7ee835-ddec-44f7-98c9-89c8ac94a2e6", "embedding": null, "metadata": {"page_label": "25", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "df687014-0152-4bb6-a745-f573a02f0c8c", "node_type": "4", "metadata": {"page_label": "25", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "abe9c7b032ad87589f8922fd2721454063b4e654c931ec857913286ff98a5aa3", "class_name": "RelatedNodeInfo"}}, "text": "1.12. TROUBLESHOOTING OPENSHIFT CONTAINER PLATFORM\nVERSION 3.11 CLUSTER IMPORT FAILURE\n1.12.1. Symptom: OpenShift Container Platform version 3.11 cluster import failure\nAfter you attempt to import a Red Hat OpenShift Container Platform version 3.11 cluster, the import fails\nwith a log message that resembles the following content:\ncustomresourcedefinition.apiextensions.k8s.io/klusterlets.operator.open-cluster-management.io\n \nconfigured\nclusterrole.rbac.authorization.k8s.io/klusterlet configured\nclusterrole.rbac.authorization.k8s.io/open-cluster-management:klusterlet-admin-aggregate-clusterrole\n \nconfigured\nclusterrolebinding.rbac.authorization.k8s.io/klusterlet configured\nnamespace/open-cluster-management-agent configured\nsecret/open-cluster-management-image-pull-credentials unchanged\nserviceaccount/klusterlet configured\ndeployment.apps/klusterlet unchanged\nklusterlet.operator.open-cluster-management.io/klusterlet configured\nError from server (BadRequest): error when creating \"STDIN\": Secret in version \"v1\" cannot be\n \nhandled as a Secret:\nv1.Secret.ObjectMeta:\nv1.ObjectMeta.TypeMeta: Kind: Data: decode base64: illegal base64 data at input byte 1313, error\n \nfound in #10 byte of ...|dhruy45=\"},\"kind\":\"|..., bigger context\n \n...|tye56u56u568yuo7i67i67i67o556574i\"},\"kind\":\"Secret\",\"metadata\":{\"annotations\":{\"kube|...\n1.12.2. Identifying the problem: OpenShift Container Platform version 3.11 cluster\nimport failure\nThis often occurs because the installed version of the \nkubectl\n command-line tool is 1.11, or earlier. Run\nthe following command to see which version of the \nkubectl\n command-line tool you are running:\nkubectl version\nIf the returned data lists version 1.11, or earlier, complete one of the fixes in \nResolving the problem:\nOpenShift Container Platform version 3.11 cluster import failure\n.\n1.12.3. Resolving the problem: OpenShift Container Platform version 3.11 cluster\nimport failure\nYou can resolve this issue by completing one of the following procedures:\nInstall the latest version of the \nkubectl\n command-line tool.\n1\n. \nDownload the latest version of the \nkubectl\n tool from \nInstall and Set Up kubectl\n in the\nKubernetes documentation.\n2\n. \nImport the cluster again after upgrading your \nkubectl\n tool.\nRun a file that contains the import command.\n1\n. \nStart the procedure in \nImporting a managed cluster with the CLI\n.\n2\n. \nWhen you create the command to import your cluster, copy that command into a YAML file\nCHAPTER 1. TROUBLESHOOTING\n21", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2486, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "753f7f61-a993-4eeb-8e71-cf99c152f5d2": {"__data__": {"id_": "753f7f61-a993-4eeb-8e71-cf99c152f5d2", "embedding": null, "metadata": {"page_label": "26", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "29fecb31-b7ba-43fa-94f6-aae61f96e599", "node_type": "4", "metadata": {"page_label": "26", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "81921972f2f23684906dfabc9556dfc9520f7e8bd3766daee8458efaa37a6943", "class_name": "RelatedNodeInfo"}}, "text": "2\n. \nWhen you create the command to import your cluster, copy that command into a YAML file\nnamed \nimport.yaml\n.\n3\n. \nRun the following command to import the cluster again from the file:\noc apply -f import.yaml\n1.13. TROUBLESHOOTING IMPORTED CLUSTERS OFFLINE AFTER\nCERTIFICATE CHANGE\nInstalling a custom \napiserver\n certificate is supported, but one or more clusters that were imported\nbefore you changed the certificate information are in \noffline\n status.\n1.13.1. Symptom: Clusters offline after certificate change\nAfter you complete the procedure for updating a certificate secret, one or more of your clusters that\nwere online now display \noffline\n status in the console.\n1.13.2. Identifying the problem: Clusters offline after certificate change\nAfter updating the information for a custom API server certificate, clusters that were imported and\nrunning before the new certificate are now in an \noffline\n state.\nThe errors that indicate that the certificate is the problem are found in the logs for the pods in the \nopen-cluster-management-agent\n namespace of the offline managed cluster. The following examples\nare similar to the errors that are displayed in the logs:\nSee the following \nwork-agent\n log:\nE0917 03:04:05.874759       1 manifestwork_controller.go:179] Reconcile work test-1-klusterlet-\naddon-workmgr fails with err: Failed to update work status with err Get \"https://api.aaa-\nocp.dev02.location.com:6443/apis/cluster.management.io/v1/namespaces/test-1/manifestworks/test-\n1-klusterlet-addon-workmgr\": x509: certificate signed by unknown authority\nE0917 03:04:05.874887       1 base_controller.go:231] \"ManifestWorkAgent\" controller failed to sync\n \n\"test-1-klusterlet-addon-workmgr\", err: Failed to update work status with err Get \"api.aaa-\nocp.dev02.location.com:6443/apis/cluster.management.io/v1/namespaces/test-1/manifestworks/test-\n1-klusterlet-addon-workmgr\": x509: certificate signed by unknown authority\nE0917 03:04:37.245859       1 reflector.go:127] k8s.io/client-go@v0.19.0/tools/cache/reflector.go:156:\n \nFailed to watch *v1.ManifestWork: failed to list *v1.ManifestWork: Get \"api.aaa-\nocp.dev02.location.com:6443/apis/cluster.management.io/v1/namespaces/test-1/manifestworks?\nresourceVersion=607424\": x509: certificate signed by unknown authority\nSee the following \nregistration-agent\n log:\nI0917 02:27:41.525026       1 event.go:282] Event(v1.ObjectReference{Kind:\"Namespace\",\n \nNamespace:\"open-cluster-management-agent\", Name:\"open-cluster-management-agent\", UID:\"\",\n \nAPIVersion:\"v1\", ResourceVersion:\"\", FieldPath:\"\"}): type: 'Normal' reason:\n \n'ManagedClusterAvailableConditionUpdated' update managed cluster \"test-1\" available condition to\n \n\"True\", due to \"Managed cluster is available\"\nE0917 02:58:26.315984       1 reflector.go:127] k8s.io/client-go@v0.19.0/tools/cache/reflector.go:156:\n \nFailed to watch *v1beta1.CertificateSigningRequest: Get \"https://api.aaa-\nocp.dev02.location.com:6443/apis/cluster.management.io/v1/managedclusters?\nallowWatchBookmarks=true&fieldSelector=metadata.name%3Dtest-\n1&resourceVersion=607408&timeout=9m33s&timeoutSeconds=573&watch=true\"\": x509: certificate\n \nRed Hat Advanced Cluster Management for Kubernetes 2.11 Troubleshooting\n22", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3209, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a16e535e-f070-46e7-8877-d02518b7eb91": {"__data__": {"id_": "a16e535e-f070-46e7-8877-d02518b7eb91", "embedding": null, "metadata": {"page_label": "27", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dd226be5-5385-4062-bc26-230467c3f604", "node_type": "4", "metadata": {"page_label": "27", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "4b4a99c380f72567262237b6306c4eda4dcd1d3949235c1c867982fdb3c6b829", "class_name": "RelatedNodeInfo"}}, "text": "signed by unknown authority\nE0917 02:58:26.598343       1 reflector.go:127] k8s.io/client-go@v0.19.0/tools/cache/reflector.go:156:\n \nFailed to watch *v1.ManagedCluster: Get \"https://api.aaa-\nocp.dev02.location.com:6443/apis/cluster.management.io/v1/managedclusters?\nallowWatchBookmarks=true&fieldSelector=metadata.name%3Dtest-\n1&resourceVersion=607408&timeout=9m33s&timeoutSeconds=573&watch=true\": x509: certificate\n \nsigned by unknown authority\nE0917 02:58:27.613963       1 reflector.go:127] k8s.io/client-go@v0.19.0/tools/cache/reflector.go:156:\n \nFailed to watch *v1.ManagedCluster: failed to list *v1.ManagedCluster: Get \"https://api.aaa-\nocp.dev02.location.com:6443/apis/cluster.management.io/v1/managedclusters?\nallowWatchBookmarks=true&fieldSelector=metadata.name%3Dtest-\n1&resourceVersion=607408&timeout=9m33s&timeoutSeconds=573&watch=true\"\": x509: certificate\n \nsigned by unknown authority\n1.13.3. Resolving the problem: Clusters offline after certificate change\nIf your managed cluster is the \nlocal-cluster\n, or your managed cluster was created by using Red Hat\nAdvanced Cluster Management for Kubernetes, you must wait 10 minutes or longer to reimport your\nmanaged cluster.\nTo reimport your managed cluster immediately, you can delete your managed cluster import secret on\nthe hub cluster and reimport it by using Red Hat Advanced Cluster Management. Run the following\ncommand:\noc delete secret -n <cluster_name> <cluster_name>-import\nReplace \n<cluster_name>\n with the name of the managed cluster that you want to import.\nIf you want to reimport a managed cluster that was imported by using Red Hat Advanced Cluster\nManagement, complete the following steps to import the managed cluster again:\n1\n. \nOn the hub cluster, recreate the managed cluster import secret by running the following\ncommand:\noc delete secret -n <cluster_name> <cluster_name>-import\nReplace \n<cluster_name>\n with the name of the managed cluster that you want to import.\n2\n. \nOn the hub cluster, expose the managed cluster import secret to a YAML file by running the\nfollowing command:\noc get secret -n <cluster_name> <cluster_name>-import -ojsonpath='{.data.import\\.yaml}' |\n \nbase64 --decode  > import.yaml\nReplace \n<cluster_name>\n with the name of the managed cluster that you want to import.\n3\n. \nOn the managed cluster, apply the \nimport.yaml\n file by running the following command:\noc apply -f import.yaml\nNote:\n The previous steps do not detach the managed cluster from the hub cluster. The steps update the\nrequired manifests with current settings on the managed cluster, including the new certificate\ninformation.\nCHAPTER 1. TROUBLESHOOTING\n23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2634, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9dbcbebd-4af0-486a-b73d-d9623a8b3ee2": {"__data__": {"id_": "9dbcbebd-4af0-486a-b73d-d9623a8b3ee2", "embedding": null, "metadata": {"page_label": "28", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a9da59e6-ce38-4cdb-aba8-0ee4a3ebcf30", "node_type": "4", "metadata": {"page_label": "28", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "161c8eab8c4f75bebf5496cd12d30472464a697a1054a1629b255e72bc1e5a20", "class_name": "RelatedNodeInfo"}}, "text": "1.14. NAMESPACE REMAINS AFTER DELETING A CLUSTER\nWhen you remove a managed cluster, the namespace is normally removed as part of the cluster removal\nprocess. In rare cases, the namespace remains with some artifacts in it. In that case, you must manually\nremove the namespace.\n1.14.1. Symptom: Namespace remains after deleting a cluster\nAfter removing a managed cluster, the namespace is not removed.\n1.14.2. Resolving the problem: Namespace remains after deleting a cluster\nComplete the following steps to remove the namespace manually:\n1\n. \nRun the following command to produce a list of the resources that remain in the <cluster_name>\nnamespace:\noc api-resources --verbs=list --namespaced -o name | grep -E\n \n'^secrets|^serviceaccounts|^managedclusteraddons|^roles|^rolebindings|^manifestworks|^leases\n|^managedclusterinfo|^appliedmanifestworks'|^clusteroauths' | xargs -n 1 oc get --show-kind -\n-ignore-not-found -n <cluster_name>\nReplace \ncluster_name\n with the name of the namespace for the cluster that you attempted to\nremove.\n2\n. \nDelete each identified resource on the list that does not have a status of \nDelete\n by entering the\nfollowing command to edit the list:\noc edit <resource_kind> <resource_name> -n <namespace>\nReplace \nresource_kind\n with the kind of the resource. Replace \nresource_name\n with the name\nof the resource. Replace \nnamespace\n with the name of the namespace of the resource.\n3\n. \nLocate the \nfinalizer\n attribute in the in the metadata.\n4\n. \nDelete the non-Kubernetes finalizers by using the vi editor \ndd\n command.\n5\n. \nSave the list and exit the \nvi\n editor by entering the \n:wq\n command.\n6\n. \nDelete the namespace by entering the following command:\noc delete ns <cluster-name>\nReplace \ncluster-name\n with the name of the namespace that you are trying to delete.\n1.15. AUTO-IMPORT-SECRET-EXISTS ERROR WHEN IMPORTING A\nCLUSTER\nYour cluster import fails with an error message that reads: auto import secret exists.\n1.15.1. Symptom: Auto import secret exists error when importing a cluster\nWhen importing a hive cluster for management, an \nauto-import-secret already exists\n error is\ndisplayed.\nRed Hat Advanced Cluster Management for Kubernetes 2.11 Troubleshooting\n24", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2201, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be8fabb0-75ab-4fd0-a051-7bfafec82d32": {"__data__": {"id_": "be8fabb0-75ab-4fd0-a051-7bfafec82d32", "embedding": null, "metadata": {"page_label": "29", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "62d7f706-6a87-4ffb-bef9-57f43469cf87", "node_type": "4", "metadata": {"page_label": "29", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "3f6e2b9135c3bc30af6e0e59a68fed82b7b342536396b7ce3b02e8454e719d7c", "class_name": "RelatedNodeInfo"}}, "text": "1.15.2. Resolving the problem: Auto-import-secret-exists error when importing a\ncluster\nThis problem occurs when you attempt to import a cluster that was previously managed by Red Hat\nAdvanced Cluster Management. When this happens, the secrets conflict when you try to reimport the\ncluster.\nTo work around this problem, complete the following steps:\n1\n. \nTo manually delete the existing \nauto-import-secret\n, run the following command on the hub\ncluster:\noc delete secret auto-import-secret -n <cluster-namespace>\nReplace \ncluster-namespace\n with the namespace of your cluster.\n2\n. \nImport your cluster again by using the procedure in \nCluster import introduction\n.\n1.16. TROUBLESHOOTING THE CINDER CONTAINER STORAGE\nINTERFACE (CSI) DRIVER FOR VOLSYNC\nIf you use VolSync or use a default setting in a cinder Container Storage Interface (CSI) driver, you\nmight encounter errors for the PVC that is in use.\n1.16.1. Symptom: \nVolumesnapshot\n error state\nYou can configure a VolSync \nReplicationSource\n or \nReplicationDestination\n to use snapshots. Also, you\ncan configure the \nstorageclass\n and \nvolumesnapshotclass\n in the \nReplicationSource\n and \nReplicationDestination\n. There is a parameter on the cinder \nvolumesnapshotclass\n called \nforce-create\nwith a default value of \nfalse\n. This \nforce-create\n parameter on the \nvolumesnapshotclass\n means cinder\ndoes not allow the \nvolumesnapshot\n to be taken of a PVC in use. As a result, the \nvolumesnapshot\n is in\nan error state.\n1.16.2. Resolving the problem: Setting the parameter to \ntrue\n1\n. \nCreate a new \nvolumesnapshotclass\n for the cinder CSI driver.\n2\n. \nChange the paramater, \nforce-create\n, to \ntrue\n. See the following sample YAML:\n1.17. TROUBLESHOOTING WITH THE \nMUST-GATHER\n COMMAND\napiVersion:\n snapshot.storage.k8s.io/v1\ndeletionPolicy:\n Delete\ndriver:\n cinder.csi.openstack.org\nkind:\n VolumeSnapshotClass\nmetadata:\n  annotations:\n    snapshot.storage.kubernetes.io/is-default-class: \n'true'\n  name:\n standard-csi\nparameters:\n  force-create:\n \n'true'\nCHAPTER 1. TROUBLESHOOTING\n25", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2041, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb560299-f2ae-4b86-9127-e2a946003fd4": {"__data__": {"id_": "eb560299-f2ae-4b86-9127-e2a946003fd4", "embedding": null, "metadata": {"page_label": "30", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1ecd8271-b8b8-4c97-aece-bf328cac961d", "node_type": "4", "metadata": {"page_label": "30", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "87911dbe969f0dcdf0e61c899edc02a98c3d882aa20a545eb1f2c3dd5aa8d0ea", "class_name": "RelatedNodeInfo"}}, "text": "1.17.1. Symptom: Errors with multicluster global hub\nYou might experience various errors with multicluster global hub. You can run the \nmust-gather\ncommand for troubleshooting issues with multicluster global hub.\n1.17.2. Resolving the problem: Running the \nmust-gather\n command for dubugging\nRun the \nmust-gather\n command to gather details, logs, and take steps in debugging issues. This\ndebugging information is also useful when you open a support request. The \noc adm must-gather\n CLI\ncommand collects the information from your cluster that is often needed for debugging issues,\nincluding:\nResource definitions\nService logs\n1.17.2.1. Prerequisites\nYou must meet the following prerequisites to run the \nmust-gather\n command:\nAccess to the global hub and managed hub clusters as a user with the \ncluster-admin\n role.\nThe OpenShift Container Platform CLI (oc) installed.\n1.17.2.2. Running the must-gather command\nComplete the following procedure to collect information by using the must-gather command:\n1\n. \nLearn about the \nmust-gather\n command and install the prerequisites that you need by reading\nthe \nGathering data about your cluster\n in the OpenShift Container Platform documentation.\n2\n. \nLog in to your global hub cluster. For the typical use case, run the following command while you\nare logged into your global hub cluster:\noc adm must-gather --image=quay.io/stolostron/must-gather:SNAPSHOTNAME\nIf you want to check your managed hub clusters, run the \nmust-gather\n command on those\nclusters.\n3\n. \nOptional: If you want to save the results in a the \nSOMENAME\n directory, you can run the\nfollowing command instead of the one in the previous step:\noc adm must-gather --image=quay.io/stolostron/must-gather:SNAPSHOTNAME --dest-dir=\n<SOMENAME> ; tar -cvzf <SOMENAME>.tgz <SOMENAME>\nYou can specify a different name for the directory.\nNote:\n The command includes the required additions to create a gzipped tarball file.\nThe following information is collected from the \nmust-gather\n command:\nTwo peer levels: \ncluster-scoped-resources\n and \nnamespaces\n resources.\nSub-level for each: API group for the custom resource definitions for both cluster-scope and\nnamespace-scoped resources.\nRed Hat Advanced Cluster Management for Kubernetes 2.11 Troubleshooting\n26", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2262, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0c7ad3c2-da84-41a2-9b9e-f689560c466d": {"__data__": {"id_": "0c7ad3c2-da84-41a2-9b9e-f689560c466d", "embedding": null, "metadata": {"page_label": "31", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a5d7308c-695f-43e1-b065-e354ceadc846", "node_type": "4", "metadata": {"page_label": "31", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "6ef8ebcbd09d0c9de3188216e1e10d26e55c4355a0ccd45c8cfcb17f6f0307f0", "class_name": "RelatedNodeInfo"}}, "text": "Next level for each: YAML file sorted by kind.\nFor the global hub cluster, you can check the \nPostgresCluster\n and \nKafka\n in the \nnamespaces\nresources.\nFor the global hub cluster, you can check the multicluster global hub related pods and logs in \npods\n of \nnamespaces\n resources.\nFor the managed hub cluster, you can check the multicluster global hub agent pods and logs in \npods\n of \nnamespaces\n resources.\n1.18. TROUBLESHOOTING BY ACCESSING THE POSTGRESQL\nDATABASE\n1.18.1. Symptom: Errors with multicluster global hub\nYou might experience various errors with multicluster global hub. You can access the provisioned\nPostgreSQL database to view messages that might be helpful for troubleshooting issues with\nmulticluster global hub.\n1.18.2. Resolving the problem: Accessing the PostgresSQL database\nThere are two ways to access the provisioned PostgreSQL database.\nUsing the \nClusterIP\n service\noc exec -it multicluster-global-hub-postgres-0 -c multicluster-global-hub-postgres -n\n \nmulticluster-global-hub -- psql -U postgres -d hoh\n# Or access the database installed by crunchy operator\noc exec -it $(kubectl get pods -n multicluster-global-hub -l postgres-\noperator.crunchydata.com/role=master -o jsonpath='{.items..metadata.name}') -c database -n\n \nmulticluster-global-hub -- psql -U postgres -d hoh -c \"SELECT 1\"\nLoadBalancer\nExpose the service type to \nLoadBalancer\n provisioned by default:\ncat <<EOF | oc apply -f -\napiVersion: v1\nkind: Service\nmetadata:\n  name: multicluster-global-hub-postgres-lb\n  namespace: multicluster-global-hub\nspec:\n  ports:\n  - name: postgres\n    port: 5432\n    protocol: TCP\n    targetPort: 5432\n  selector:\n    name: multicluster-global-hub-postgres\n  type: LoadBalancer\nEOF\nCHAPTER 1. TROUBLESHOOTING\n27", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1742, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7602244-50b6-43af-90f9-d486e0d84232": {"__data__": {"id_": "c7602244-50b6-43af-90f9-d486e0d84232", "embedding": null, "metadata": {"page_label": "32", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8196e504-6b96-4f71-9900-9577678576d2", "node_type": "4", "metadata": {"page_label": "32", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "5cdae6c448b81c9bfee4f5db6f3d9f8038941a5b221277e7162fe889b4f5433f", "class_name": "RelatedNodeInfo"}}, "text": "Run the following command to get your the credentials:\n# Host\noc get svc postgres-ha -ojsonpath='{.status.loadBalancer.ingress[0].hostname}'\n# Password\noc get secrets -n multicluster-global-hub postgres-pguser-postgres -o go-\ntemplate='{{index (.data) \"password\" | base64decode}}'\nExpose the service type to \nLoadBalancer\n provisioned by crunchy operator:\noc patch postgrescluster postgres -n multicluster-global-hub -p '{\"spec\":{\"service\":\n{\"type\":\"LoadBalancer\"}}}'  --type merge\nRun the following command to get your the credentials:\n# Host\noc get svc -n multicluster-global-hub postgres-ha -\nojsonpath='{.status.loadBalancer.ingress[0].hostname}'\n# Username\noc get secrets -n multicluster-global-hub postgres-pguser-postgres -o go-\ntemplate='{{index (.data) \"user\" | base64decode}}'\n# Password\noc get secrets -n multicluster-global-hub postgres-pguser-postgres -o go-\ntemplate='{{index (.data) \"password\" | base64decode}}'\n# Database\noc get secrets -n multicluster-global-hub postgres-pguser-postgres -o go-\ntemplate='{{index (.data) \"dbname\" | base64decode}}'\n1.19. TROUBLESHOOTING BY USING THE DATABASE DUMP AND\nRESTORE\nIn a production environment, back up your PostgreSQL database regularly as a database management\ntask. The backup can also be used for debugging the multicluster global hub.\n1.19.1. Symptom: Errors with multicluster global hub\nYou might experience various errors with multicluster global hub. You can use the database dump and\nrestore for troubleshooting issues with multicluster global hub.\n1.19.2. Resolving the problem: Dumping the output of the database for dubugging\nSometimes you need to dump the output in the multicluster global hub database to debug a problem.\nThe PostgreSQL database provides the \npg_dump\n command line tool to dump the contents of the\ndatabase. To dump data from localhost database server, run the following command:\npg_dump hoh > hoh.sql\nTo dump the multicluster global hub database located on a remote server with compressed format, use\nRed Hat Advanced Cluster Management for Kubernetes 2.11 Troubleshooting\n28", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2067, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "267d064f-3ed4-4e37-a7d2-8f01f11ee89c": {"__data__": {"id_": "267d064f-3ed4-4e37-a7d2-8f01f11ee89c", "embedding": null, "metadata": {"page_label": "33", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18d63416-8aa1-49f8-be89-a160ced2e4a8", "node_type": "4", "metadata": {"page_label": "33", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "5ceb0107fd284790838bc343cc7523461924e2eb10cb712da09ba02a1ad83718", "class_name": "RelatedNodeInfo"}}, "text": "To dump the multicluster global hub database located on a remote server with compressed format, use\nthe command-line options to control the connection details, as shown in the following example:\npg_dump -h my.host.com -p 5432 -U postgres -F t hoh -f hoh-$(date +%d-%m-%y_%H-%M).tar\n1.19.3. Resolving the problem: Restore database from dump\nTo restore a PostgreSQL database, you can use the \npsql\n or \npg_restore\n command line tools. The \npsql\ntool is used to restore plain text files created by \npg_dump\n:\npsql -h another.host.com -p 5432 -U postgres -d hoh < hoh.sql\nThe \npg_restore\n tool is used to restore a PostgreSQL database from an archive created by \npg_dump\n in\none of the non-plain-text formats (custom, tar, or directory):\npg_restore -h another.host.com -p 5432 -U postgres -d hoh hoh-$(date +%d-%m-%y_%H-%M).tar\n1.20. TROUBLESHOOTING CLUSTER STATUS CHANGING FROM\nOFFLINE TO AVAILABLE\nThe status of the managed cluster alternates between \noffline\n and \navailable\n without any manual change\nto the environment or cluster.\n1.20.1. Symptom: Cluster status changing from offline to available\nWhen the network that connects the managed cluster to the hub cluster is unstable, the status of the\nmanaged cluster that is reported by the hub cluster cycles between \noffline\n and \navailable\n.\nThe connection between the hub cluster and managed cluster is maintained through a lease that is\nvalidated at the \nleaseDurationSeconds\n interval value. If the lease is not validated within five\nconsecutive attempts of the \nleaseDurationSeconds\n value, then the cluster is marked \noffline\n.\nFor example, the cluster is marked \noffline\n after five minutes with a \nleaseDurationSeconds\n interval of \n60 seconds\n. This configuration can be inadequate for reasons such as connectivity issues or latency,\ncausing instability.\n1.20.2. Resolving the problem: Cluster status changing from offline to available\nThe five validation attempts is default and cannot be changed, but you can change the \nleaseDurationSeconds\n interval.\nDetermine the amount of time, in minutes, that you want the cluster to be marked as \noffline\n, then\nmultiply that value by 60 to convert to seconds. Then divide by the default five number of attempts. The\nresult is your \nleaseDurationSeconds\n value.\n1\n. \nEdit your \nManagedCluster\n specification on the hub cluster by entering the following command,\nbut replace \ncluster-name\n with the name of your managed cluster:\noc edit managedcluster <cluster-name>\n2\n. \nIncrease the value of \nleaseDurationSeconds\n in your \nManagedCluster\n specification, as seen in\nthe following sample YAML:\nCHAPTER 1. TROUBLESHOOTING\n29", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2626, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35683573-feb1-444b-b76d-0364e0af53fe": {"__data__": {"id_": "35683573-feb1-444b-b76d-0364e0af53fe", "embedding": null, "metadata": {"page_label": "34", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "98b38f52-7ea3-47ce-90a5-8675baca361b", "node_type": "4", "metadata": {"page_label": "34", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "21085fc25bae1e43cdfbb90ab67834af7a4cde60b0cdc9bf88b2c32ec2d859ef", "class_name": "RelatedNodeInfo"}}, "text": "3\n. \nSave and apply the file.\n1.21. TROUBLESHOOTING CLUSTER IN CONSOLE WITH PENDING OR\nFAILED STATUS\nIf you observe \nPending\n status or \nFailed\n status in the console for a cluster you created, follow the\nprocedure to troubleshoot the problem.\n1.21.1. Symptom: Cluster in console with pending or failed status\nAfter creating a new cluster by using the Red Hat Advanced Cluster Management for Kubernetes\nconsole, the cluster does not progress beyond the status of \nPending\n or displays \nFailed\n status.\n1.21.2. Identifying the problem: Cluster in console with pending or failed status\nIf the cluster displays \nFailed\n status, navigate to the details page for the cluster and follow the link to the\nlogs provided. If no logs are found or the cluster displays \nPending\n status, continue with the following\nprocedure to check for logs:\nProcedure 1\n1\n. \nRun the following command on the hub cluster to view the names of the Kubernetes pods\nthat were created in the namespace for the new cluster:\noc get pod -n <new_cluster_name>\nReplace \nnew_cluster_name\n with the name of the cluster that you created.\n2\n. \nIf no pod that contains the string \nprovision\n in the name is listed, continue with Procedure 2.\nIf there is a pod with \nprovision\n in the title, run the following command on the hub cluster to\nview the logs of that pod:\noc logs <new_cluster_name_provision_pod_name> -n <new_cluster_name> -c hive\nReplace \nnew_cluster_name_provision_pod_name\n with the name of the cluster that you\ncreated, followed by the pod name that contains \nprovision\n.\n3\n. \nSearch for errors in the logs that might explain the cause of the problem.\nProcedure 2\nIf there is not a pod with \nprovision\n in its name, the problem occurred earlier in the process.\nComplete the following procedure to view the logs:\n1\n. \nRun the following command on the hub cluster:\noc describe clusterdeployments -n <new_cluster_name>\napiVersion:\n cluster.open-cluster-management.io/v1\nkind:\n ManagedCluster\nmetadata:\n  name:\n <cluster-name\n>\nspec:\n  hubAcceptsClient:\n \ntrue\n  leaseDurationSeconds:\n \n60\nRed Hat Advanced Cluster Management for Kubernetes 2.11 Troubleshooting\n30", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2133, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11dc20d7-a40f-4ce1-adfc-6fba5ce7badb": {"__data__": {"id_": "11dc20d7-a40f-4ce1-adfc-6fba5ce7badb", "embedding": null, "metadata": {"page_label": "35", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7420811d-bd3f-432c-8ddc-f5d9279985ba", "node_type": "4", "metadata": {"page_label": "35", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "f10e93b7dc5f2fbb7b3a5efe026ebc622da0415504e5627cf84cb4ac34e2df71", "class_name": "RelatedNodeInfo"}}, "text": "Replace \nnew_cluster_name\n with the name of the cluster that you created. For more\ninformation about cluster installation logs, see \nGathering installation logs\n in the Red Hat\nOpenShift documentation.\n2\n. \nSee if there is additional information about the problem in the \nStatus.Conditions.Message\nand \nStatus.Conditions.Reason\n entries of the resource.\n1.21.3. Resolving the problem: Cluster in console with pending or failed status\nAfter you identify the errors in the logs, determine how to resolve the errors before you destroy the\ncluster and create it again.\nThe following example provides a possible log error of selecting an unsupported zone, and the actions\nthat are required to resolve it:\nNo subnets provided for zones\nWhen you created your cluster, you selected one or more zones within a region that are not supported.\nComplete one of the following actions when you recreate your cluster to resolve the issue:\nSelect a different zone within the region.\nOmit the zone that does not provide the support, if you have other zones listed.\nSelect a different region for your cluster.\nAfter determining the issues from the log, destroy the cluster and recreate it.\nSee \nCluster creation introduction\n for more information about creating a cluster.\n1.22. TROUBLESHOOTING GRAFANA\nWhen you query some time-consuming metrics in the Grafana explorer, you might encounter a \nGateway\n \nTime-out\n error.\n1.22.1. Symptom: Grafana explorer gateway timeout\nIf you hit the \nGateway Time-out\n error when you query some time-consuming metrics in the Grafana\nexplorer, it is possible that the timeout is caused by the Grafana in the \nopen-cluster-management-\nobservability\n namespace.\n1.22.2. Resolving the problem: Configure the Grafana\nIf you have this problem, complete the following steps:\n1\n. \nVerify that the default configuration of Grafana has expected timeout settings:\na\n. \nTo verify that the default timeout setting of Grafana, run the following command:\noc get secret grafana-config -n open-cluster-management-observability -o jsonpath=\"\n{.data.grafana\\.ini}\" | base64 -d | grep dataproxy -A 4\nThe following timeout settings should be displayed:\nCHAPTER 1. TROUBLESHOOTING\n31", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2178, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8561eb5d-de5f-425d-9e98-31baea6b7d47": {"__data__": {"id_": "8561eb5d-de5f-425d-9e98-31baea6b7d47", "embedding": null, "metadata": {"page_label": "36", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38ed601c-c93d-4226-9d3c-259a036dfbf5", "node_type": "4", "metadata": {"page_label": "36", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "30459dea3fd7f90312911a76d5988c1db10d725fb6ee75b140183c03ff108b0f", "class_name": "RelatedNodeInfo"}}, "text": "[dataproxy]\ntimeout = 300\ndial_timeout = 30\nkeep_alive_seconds = 300\nb\n. \nTo verify the default data source query timeout for Grafana, run the following command:\noc get secret/grafana-datasources -n open-cluster-management-observability -o\n \njsonpath=\"{.data.datasources\\.yaml}\" | base64 -d | grep queryTimeout\nThe following timeout settings should be displayed:\nqueryTimeout: 300s\n2\n. \nIf you verified the default configuration of Grafana has expected timeout settings, then you can\nconfigure the Grafana in the \nopen-cluster-management-observability\n namespace by running\nthe following command:\noc annotate route grafana -n open-cluster-management-observability --overwrite\n \nhaproxy.router.openshift.io/timeout=300s\nRefresh the Grafana page and try to query the metrics again. The \nGateway Time-out\n error is no longer\ndisplayed.\n1.23. TROUBLESHOOTING LOCAL CLUSTER NOT SELECTED WITH\nPLACEMENT RULE\nThe managed clusters are selected with a placement rule, but the \nlocal-cluster\n, which is a hub cluster\nthat is also managed, is not selected. The placement rule user is not granted permission to get the \nmanagedcluster\n resources in the \nlocal-cluster\n namespace.\n1.23.1. Symptom: Troubleshooting local cluster not selected as a managed cluster\nAll managed clusters are selected with a placement rule, but the \nlocal-cluster\n is not. The placement rule\nuser is not granted permission to get the \nmanagedcluster\n resources in the \nlocal-cluster\n namespace.\n1.23.2. Resolving the problem: Troubleshooting local cluster not selected as a\nmanaged cluster\nDeprecated:\n \nPlacementRule\nTo resolve this issue, you need to grant the \nmanagedcluster\n administrative permission in the \nlocal-\ncluster\n namespace. Complete the following steps:\n1\n. \nConfirm that the list of managed clusters does include \nlocal-cluster\n, and that the placement\nrule \ndecisions\n list does not display the \nlocal-cluster\n. Run the following command and view the\nresults:\n% oc get managedclusters\nSee in the sample output that \nlocal-cluster\n is joined, but it is not in the YAML for \nPlacementRule\n:\nRed Hat Advanced Cluster Management for Kubernetes 2.11 Troubleshooting\n32", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2147, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13e7ff9a-94d8-449f-ab2a-5d5bcff742de": {"__data__": {"id_": "13e7ff9a-94d8-449f-ab2a-5d5bcff742de", "embedding": null, "metadata": {"page_label": "37", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "62c1d69c-dc87-44d1-b4ff-98e3c659f7cc", "node_type": "4", "metadata": {"page_label": "37", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "50c61b423fc46fc1fb69b3e858188d7bc03f2714ccf6306f7034e64a9942fd1f", "class_name": "RelatedNodeInfo"}}, "text": "NAME            HUB ACCEPTED   MANAGED CLUSTER URLS   JOINED   AVAILABLE\n   \nAGE\nlocal-cluster   true                                  True     True        56d\ncluster1        true                                  True     True        16h\n2\n. \nCreate a \nRole\n in your YAML file to grant the \nmanagedcluster\n administrative permission in the \nlocal-cluster\n namespace. See the following example:\n3\n. \nCreate a \nRoleBinding\n resource to grant the placement rule user access to the \nlocal-cluster\nnamespace. See the following example:\n1.24. TROUBLESHOOTING APPLICATION KUBERNETES DEPLOYMENT\nVERSION\napiVersion:\n apps.open-cluster-management.io/v1\nkind:\n PlacementRule\nmetadata:\n  name:\n all-ready-clusters\n  namespace:\n default\nspec:\n  clusterSelector:\n {}\nstatus:\n  decisions:\n  - clusterName:\n cluster1\n    clusterNamespace:\n cluster1\napiVersion:\n rbac.authorization.k8s.io/v1\nkind:\n Role\nmetadata:\n  name:\n managedcluster-admin-user-zisis\n  namespace:\n local-cluster\nrules:\n- apiGroups:\n  -\n cluster.open-cluster-management.io\n  resources:\n  -\n managedclusters\n  verbs:\n  -\n get\napiVersion:\n rbac.authorization.k8s.io/v1\nkind:\n RoleBinding\nmetadata:\n  name:\n managedcluster-admin-user-zisis\n  namespace:\n local-cluster\nroleRef:\n  apiGroup:\n rbac.authorization.k8s.io\n  kind:\n Role\n  name:\n managedcluster-admin-user-zisis\n  namespace:\n local-cluster\nsubjects:\n- kind:\n User\n  name:\n zisis\n  apiGroup:\n rbac.authorization.k8s.io\nCHAPTER 1. TROUBLESHOOTING\n33", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1457, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1896bc7-71d6-47be-b640-f9d50c6fa7fe": {"__data__": {"id_": "e1896bc7-71d6-47be-b640-f9d50c6fa7fe", "embedding": null, "metadata": {"page_label": "38", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8b7630b8-439f-47ec-afa9-804e7bafe506", "node_type": "4", "metadata": {"page_label": "38", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "7f889c377e46b2d76292bf8cc5c53e170b71a30eaf581b23a0d96707572b2568", "class_name": "RelatedNodeInfo"}}, "text": "A managed cluster with a deprecated Kubernetes \napiVersion\n might not be supported. See the\nKubernetes issue\n for more details about the deprecated API version.\n1.24.1. Symptom: Application deployment version\nIf one or more of your application resources in the Subscription YAML file uses the deprecated API, you\nmight receive an error similar to the following error:\nfailed to install release: unable to build kubernetes objects from release manifest: unable to recognize\n \n\"\": no matches for\nkind \"Deployment\" in version \"extensions/v1beta1\"\nOr with new Kubernetes API version in your YAML file named \nold.yaml\n for instance, you might receive\nthe following error:\nerror: unable to recognize \"old.yaml\": no matches for kind \"Deployment\" in version\n \n\"deployment/v1beta1\"\n1.24.2. Resolving the problem: Application deployment version\n1\n. \nUpdate the \napiVersion\n in the resource. For example, if the error displays for \nDeployment\n kind in\nthe subscription YAML file, you need to update the \napiVersion\n from \nextensions/v1beta1\n to \napps/v1\n.\nSee the following example:\n2\n. \nVerify the available versions by running the following command on the managed cluster:\nkubectl explain <resource>\n3\n. \nCheck for \nVERSION\n.\n1.25. TROUBLESHOOTING KLUSTERLET WITH DEGRADED\nCONDITIONS\nThe Klusterlet degraded conditions can help to diagnose the status of Klusterlet agents on managed\ncluster. If a Klusterlet is in the degraded condition, the Klusterlet agents on managed cluster might have\nerrors that need to be troubleshooted. See the following information for Klusterlet degraded conditions\nthat are set to \nTrue\n.\n1.25.1. Symptom: Klusterlet is in the degraded condition\nAfter deploying a Klusterlet on managed cluster, the \nKlusterletRegistrationDegraded\n or \nKlusterletWorkDegraded\n condition displays a status of \nTrue\n.\n1.25.2. Identifying the problem: Klusterlet is in the degraded condition\n1\n. \nRun the following command on the managed cluster to view the Klusterlet status:\napiVersion:\n apps/v1\nkind:\n Deployment\nRed Hat Advanced Cluster Management for Kubernetes 2.11 Troubleshooting\n34", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2090, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b70af8eb-46fe-4794-bb54-f2b6a1d67e93": {"__data__": {"id_": "b70af8eb-46fe-4794-bb54-f2b6a1d67e93", "embedding": null, "metadata": {"page_label": "39", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3edc0a60-c204-4f81-a74f-f1346c63a277", "node_type": "4", "metadata": {"page_label": "39", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "155743ceb9c718b74fadf63d1a52f04ada86ba3628924050ef2d9063545c404c", "class_name": "RelatedNodeInfo"}}, "text": "kubectl get klusterlets klusterlet -oyaml\n2\n. \nCheck \nKlusterletRegistrationDegraded\n or \nKlusterletWorkDegraded\n to see if the condition is\nset to \nTrue\n. Proceed to \nResolving the problem\n for any degraded conditions that are listed.\n1.25.3. Resolving the problem: Klusterlet is in the degraded condition\nSee the following list of degraded statuses and how you can attempt to resolve those issues:\nIf the \nKlusterletRegistrationDegraded\n condition with a status of \nTrue\n and the condition reason\nis: \nBootStrapSecretMissing\n, you need create a bootstrap secret on \nopen-cluster-management-\nagent\n namespace.\nIf the \nKlusterletRegistrationDegraded\n condition displays \nTrue\n and the condition reason is a\nBootstrapSecretError\n, or \nBootstrapSecretUnauthorized\n, then the current bootstrap secret is\ninvalid. Delete the current bootstrap secret and recreate a valid bootstrap secret on \nopen-\ncluster-management-agent\n namespace.\nIf the \nKlusterletRegistrationDegraded\n and \nKlusterletWorkDegraded\n displays \nTrue\n and the\ncondition reason is \nHubKubeConfigSecretMissing\n, delete the Klusterlet and recreate it.\nIf the \nKlusterletRegistrationDegraded\n and \nKlusterletWorkDegraded\n displays \nTrue\n and the\ncondition reason is: \nClusterNameMissing\n, \nKubeConfigMissing\n, \nHubConfigSecretError\n, or\nHubConfigSecretUnauthorized\n, delete the hub cluster kubeconfig secret from \nopen-cluster-\nmanagement-agent\n namespace. The registration agent will bootstrap again to get a new hub\ncluster kubeconfig secret.\nIf the \nKlusterletRegistrationDegraded\n displays \nTrue\n and the condition reason is\nGetRegistrationDeploymentFailed\n, or \nUnavailableRegistrationPod\n, you can check the condition\nmessage to get the problem details and attempt to resolve.\nIf the \nKlusterletWorkDegraded\n displays \nTrue\n and the condition reason is\nGetWorkDeploymentFailed\n ,or \nUnavailableWorkPod\n, you can check the condition message to get\nthe problem details and attempt to resolve.\n1.26. TROUBLESHOOTING OBJECT STORAGE CHANNEL SECRET\nIf you change the \nSecretAccessKey\n, the subscription of an Object storage channel cannot pick up the\nupdated secret automatically and you receive an error.\n1.26.1. Symptom: Object storage channel secret\nThe subscription of an Object storage channel cannot pick up the updated secret automatically. This\nprevents the subscription operator from reconciliation and deploys resources from Object storage to\nthe managed cluster.\n1.26.2. Resolving the problem: Object storage channel secret\nYou need to manually input the credentials to create a secret, then refer to the secret within a channel.\n1\n. \nAnnotate the subscription CR in order to generate a reconcile single to subscription operator.\nSee the following \ndata\n specification:\napiVersion:\n apps.open-cluster-management.io/v1\nCHAPTER 1. TROUBLESHOOTING\n35", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2818, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f0e8304-1fa6-402e-9ab8-fb75afdc30c7": {"__data__": {"id_": "0f0e8304-1fa6-402e-9ab8-fb75afdc30c7", "embedding": null, "metadata": {"page_label": "40", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8b944b41-b771-4ce2-bc6b-c529f641b6a8", "node_type": "4", "metadata": {"page_label": "40", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "d89a3112248c562f48a602e05ff8d03366dc406cce486fe85d34e8f564b0a223", "class_name": "RelatedNodeInfo"}}, "text": "2\n. \nRun \noc annotate\n to test:\noc annotate appsub -n <subscription-namespace> <subscription-name> test=true\nAfter you run the command, you can go to the Application console to verify that the resource is\ndeployed to the managed cluster. Or you can log in to the managed cluster to see if the application\nresource is created at the given namespace.\n1.27. TROUBLESHOOTING OBSERVABILITY\nAfter you install the observability component, the component might be stuck and an \nInstalling\n status is\ndisplayed.\n1.27.1. Symptom: MultiClusterObservability resource status stuck\nIf the observability status is stuck in an \nInstalling\n status after you install and create the Observability\ncustom resource definition (CRD), it is possible that there is no value defined for the \nspec:storageConfig:storageClass\n parameter. Alternatively, the observability component\nautomatically finds the default \nstorageClass\n, but if there is no value for the storage, the component\nremains stuck with the \nInstalling\n status.\n1.27.2. Resolving the problem: MultiClusterObservability resource status stuck\nIf you have this problem, complete the following steps:\n1\n. \nVerify that the observability components are installed:\na\n. \nTo verify that the \nmulticluster-observability-operator\n, run the following command:\nkind:\n Channel\nmetadata:\n  name:\n deva\n  namespace:\n ch-obj\n  labels:\n    name:\n obj-sub\nspec:\n  type:\n ObjectBucket\n  pathname:\n http://ec2\n-100\n-26\n-232\n-156.\ncompute\n-1.\namazonaws.com:\n9000\n/deva\n  sourceNamespaces:\n    -\n default\n  secretRef:\n    name:\n dev\n---\napiVersion:\n v1\nkind:\n Secret\nmetadata:\n  name:\n dev\n  namespace:\n ch-obj\n  labels:\n    name:\n obj-sub\ndata:\n  AccessKeyID:\n YWRtaW4=\n  SecretAccessKey:\n cGFzc3dvcmRhZG1pbg==\nRed Hat Advanced Cluster Management for Kubernetes 2.11 Troubleshooting\n36", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1802, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d795628d-b898-4be1-a204-bb47093f5136": {"__data__": {"id_": "d795628d-b898-4be1-a204-bb47093f5136", "embedding": null, "metadata": {"page_label": "41", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5590a902-cd61-4b45-b297-95b231551518", "node_type": "4", "metadata": {"page_label": "41", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "a5f0626e9957afbb12adb8bffd1b82d20803d6d8db82af073be9d696984cf579", "class_name": "RelatedNodeInfo"}}, "text": "kubectl get pods -n open-cluster-management|grep observability\nb\n. \nTo verify that the appropriate CRDs are present, run the following command:\nkubectl get crd|grep observ\nThe following CRDs must be displayed before you enable the component:\nmulticlusterobservabilities.observability.open-cluster-management.io\nobservabilityaddons.observability.open-cluster-management.io\nobservatoria.core.observatorium.io\n2\n. \nIf you create your own storageClass for a Bare Metal cluster, see \nPersistent storage using NFS\n.\n3\n. \nTo ensure that the observability component can find the default storageClass, update the \nstorageClass\n parameter in the \nmulticluster-observability-operator\n custom resource\ndefinition. Your parameter might resemble the following value:\nstorageclass.kubernetes.io/is-default-class: \"true\"\nThe observability component status is updated to a \nReady\n status when the installation is complete. If the\ninstallation fails to complete, the \nFail\n status is displayed.\n1.28. TROUBLESHOOTING OPENSHIFT MONITORING SERVICE\nObservability service in a managed cluster needs to scrape metrics from the OpenShift Container\nPlatform monitoring stack. The \nmetrics-collector\n is not installed if the OpenShift Container Platform\nmonitoring stack is not ready.\n1.28.1. Symptom: OpenShift monitoring service is not ready\nThe \nendpoint-observability-operator-x\n pod checks if the \nprometheus-k8s\n service is available in the \nopenshift-monitoring\n namespace. If the service is not present in the \nopenshift-monitoring\n namespace,\nthen the \nmetrics-collector\n is not deployed. You might receive the following error message: \nFailed to\n \nget prometheus resource\n.\n1.28.2. Resolving the problem: OpenShift monitoring service is not ready\nIf you have this problem, complete the following steps:\n1\n. \nLog in to your OpenShift Container Platform cluster.\n2\n. \nAccess the \nopenshift-monitoring\n namespace to verify that the \nprometheus-k8s\n service is\navailable.\n3\n. \nRestart \nendpoint-observability-operator-x\n pod in the \nopen-cluster-management-addon-\nobservability\n namespace of the managed cluster.\n1.29. TROUBLESHOOTING METRICS-COLLECTOR\nWhen the \nobservability-client-ca-certificate\n secret is not refreshed in the managed cluster, you might\nreceive an internal server error.\nCHAPTER 1. TROUBLESHOOTING\n37", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2301, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d922663-f4db-4263-8d16-b05fdc73436c": {"__data__": {"id_": "0d922663-f4db-4263-8d16-b05fdc73436c", "embedding": null, "metadata": {"page_label": "42", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "80137291-2a05-437d-b53b-54d977631b94", "node_type": "4", "metadata": {"page_label": "42", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "9b000f96660eacfa65db7d34ada173ae7ed8073d7d58fdcd3e34a9a5bd2d9a54", "class_name": "RelatedNodeInfo"}}, "text": "1.29.1. Symptom: metrics-collector cannot verify observability-client-ca-certificate\nThere might be a managed cluster, where the metrics are unavailable. If this is the case, you might\nreceive the following error from the \nmetrics-collector\n deployment:\nerror: response status code is 500 Internal Server Error, response body is x509: certificate signed by\n \nunknown authority (possibly because of \"crypto/rsa: verification error\" while trying to verify candidate\n \nauthority certificate \"observability-client-ca-certificate\")\n1.29.2. Resolving the problem: metrics-collector cannot verify observability-client-\nca-certificate\nIf you have this problem, complete the following steps:\n1\n. \nLog in to your managed cluster.\n2\n. \nDelete the secret named, \nobservability-controller-open-cluster-management.io-\nobservability-signer-client-cert\n that is in the \nopen-cluster-management-addon-\nobservability\n namespace. Run the following command:\noc delete secret observability-controller-open-cluster-management.io-observability-signer-\nclient-cert -n open-cluster-management-addon-observability\nNote:\n The \nobservability-controller-open-cluster-management.io-observability-signer-\nclient-cert\n is automatically recreated with new certificates.\nThe \nmetrics-collector\n deployment is recreated and the \nobservability-controller-open-cluster-\nmanagement.io-observability-signer-client-cert\n secret is updated.\n1.30. TROUBLESHOOTING POSTGRESQL SHARED MEMORY ERROR\nIf you have a large environment, you might encounter a PostgreSQL shared memory error that impacts\nsearch results and the topology view for applications.\n1.30.1. Symptom: PostgreSQL shared memory error\nAn error message resembling the following appears in the \nsearch-api\n logs: \nERROR: could not resize\n \nshared memory segment \"/PostgreSQL.1083654800\" to 25031264 bytes: No space left on device\n \n(SQLSTATE 53100)\n1.30.2. Resolving the problem: PostgreSQL shared memory error\nTo resolve the issue, update the PostgreSQL resources found in the \nsearch-postgres\n ConfigMap.\nComplete the following steps to update the resources:\n1\n. \nRun the following command to switch to the \nopen-cluster-management\n project:\noc project open-cluster-management\n2\n. \nIncrease the \nsearch-postgres\n pod memory. The following command increases the memory to \n16Gi\n:\nRed Hat Advanced Cluster Management for Kubernetes 2.11 Troubleshooting\n38", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2373, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1fb2e920-31e7-4d9e-a93d-50e7a1312a44": {"__data__": {"id_": "1fb2e920-31e7-4d9e-a93d-50e7a1312a44", "embedding": null, "metadata": {"page_label": "43", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "702be77f-6a9e-4cbb-b89c-abe017392e29", "node_type": "4", "metadata": {"page_label": "43", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "a4f3f62535a8ba5c21012d86a8e6c5ac406a3c27ab1b746ad066655a5564fb96", "class_name": "RelatedNodeInfo"}}, "text": "oc patch search -n open-cluster-management search-v2-operator --type json -p '[{\"op\": \"add\",\n \n\"path\": \"/spec/deployments/database/resources\", \"value\": {\"limits\": {\"memory\": \"16Gi\"},\n \n\"requests\": {\"memory\": \"32Mi\", \"cpu\": \"25m\"}}}]'\n3\n. \nRun the following command to prevent the search operator from overwriting your changes:\noc annotate search search-v2-operator search-pause=true\n4\n. \nRun the following command to update the resources in the \nsearch-postgres\n YAML file:\noc edit cm search-postgres -n open-cluster-management\nSee the following example for increasing resources:\nMake sure to save your changes before exiting.\n5\n. \nRun the following command to restart the \npostgres\n and \napi\n pod.\noc delete pod search-postgres-xyz search-api-xzy\n6\n. \nTo verify your changes, open the \nsearch-postgres\n YAML file and confirm that the changes you\nmade to \npostgresql.conf:\n are present by running the following command:\noc get cm search-postgres -n open-cluster-management -o yaml\nSee \nSearch customization and configurations\n for more information on adding environment variables.\n1.31. TROUBLESHOOTING SUBMARINER NOT CONNECTING AFTER\nINSTALLATION\nIf Submariner does not run correctly after you configure it, complete the following steps to diagnose the\nissue.\n1.31.1. Symptom: Submariner not connecting after installation\nYour Submariner network is not communicating after installation.\n1.31.2. Identifying the problem: Submariner not connecting after installation\nIf the network connectivity is not established after deploying Submariner, begin the troubleshooting\nsteps. Note that it might take several minutes for the processes to complete when you deploy\nSubmariner.\n1.31.3. Resolving the problem: Submariner not connecting after installation\nWhen Submariner does not run correctly after deployment, complete the following steps:\n1\n. \nCheck for the following requirements to determine whether the components of Submariner\n  postgresql.conf: |-\n    work_mem = \n'128MB'\n \n# Higher values allocate more memory\n    max_parallel_workers_per_gather = \n'0'\n \n# Disables parallel queries\n    shared_buffers = \n'1GB'\n \n# Higher values allocate more memory\nCHAPTER 1. TROUBLESHOOTING\n39", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2181, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "07ea2960-2d51-45aa-9840-5eb156a3b961": {"__data__": {"id_": "07ea2960-2d51-45aa-9840-5eb156a3b961", "embedding": null, "metadata": {"page_label": "44", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dac72528-7431-413f-bdd1-421873db0bb1", "node_type": "4", "metadata": {"page_label": "44", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "efe38d77599d15434e6c8ea8bd15671987170de8813c471befdaef4599ceae75", "class_name": "RelatedNodeInfo"}}, "text": "1\n. \nCheck for the following requirements to determine whether the components of Submariner\ndeployed correctly:\nThe \nsubmariner-addon\n pod is running in the \nopen-cluster-management\n namespace of\nyour hub cluster.\nThe following pods are running in the \nsubmariner-operator\n namespace of each managed\ncluster:\nsubmariner-addon\nsubmariner-gateway\nsubmariner-routeagent\nsubmariner-operator\nsubmariner-globalnet (only if Globalnet is enabled in the ClusterSet)\nsubmariner-lighthouse-agent\nsubmariner-lighthouse-coredns\nsubmariner-networkplugin-syncer (only if the specified CNI value is \nOVNKubernetes\n)\nsubmariner-metrics-proxy\n2\n. \nRun the \nsubctl diagnose all\n command to check the status of the required pods, with the\nexception of the \nsubmariner-addon\n pods.\n3\n. \nMake sure to run the \nmust-gather\n command to collect logs that can help with debugging\nissues.\n1.32. TROUBLESHOOTING SUBMARINER ADD-ON STATUS IS\nDEGRADED\nAfter adding the Submariner add-on to the clusters in your cluster set, the status in the \nConnection\nstatus\n, \nAgent status\n, and \nGateway nodes\n show unexpected status for the clusters.\n1.32.1. Symptom: Submariner add-on status is degraded\nYou add the Submariner add-on to the clusters in your cluster set, the following status is shown in the\nGateway nodes\n, \nAgent status\n, and \nConnection status\n for the clusters:\nGateway nodes labeled\nProgressing\n: The process to label the gateway nodes started.\nNodes not labeled\n: The gateway nodes are not labeled, possibly because the process to\nlabel them has not completed.\nNodes not labeled\n: The gateway nodes are not yet labeled, possibly because the process is\nwaiting for another process to finish.\nNodes labeled: The gateway nodes have been labeled.\nAgent status\nProgressing: The installation of the Submariner agent started.\nRed Hat Advanced Cluster Management for Kubernetes 2.11 Troubleshooting\n40", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1874, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "935c7e19-54e9-4d81-b1f0-6f9e645ae0c6": {"__data__": {"id_": "935c7e19-54e9-4d81-b1f0-6f9e645ae0c6", "embedding": null, "metadata": {"page_label": "45", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d836d318-3fb1-41b4-9eb8-ac2ad3875f99", "node_type": "4", "metadata": {"page_label": "45", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "83ab42f74557a3711368057fccccce97b2a635a619773566eed671452b92e9ee", "class_name": "RelatedNodeInfo"}}, "text": "Progressing: The installation of the Submariner agent started.\nDegraded: The Submariner agent is not running correctly, possibly because it is still in\nprogress.\nConnection status\nProgressing: The process to establish a connection with the Submariner add-on started.\nDegraded: The connection is not ready. If you just installed the add-on, the process might\nstill be in progress. If it was after the connection has already been established and running,\nthen two clusters have lost the connection to each other. When there are multiple clusters,\nall clusters display a \nDegraded\n status if any of the clusters is in adisconnected state.\nIt will also show which clusters are connected, and which ones are disconnected.\n1.32.2. Resolving the problem: Submariner add-on status is degraded\nThe degraded status often resolves itself as the processes complete. You can see the current\nstep of the process by clicking the status in the table. You can use that information to\ndetermine whether the process is finished and you need to take other troubleshooting steps.\nFor an issue that does not resolve itself, complete the following steps to troubleshoot the\nproblem:\n1\n. \nYou can use the \ndiagnose\n command with the \nsubctl\n utility to run some tests on the\nSubmariner connections when the following conditions exist:\na\n. \nThe \nAgent status\n or \nConnection status\n is in a \nDegraded\n state. The \ndiagnose\n command\nprovides detailed analysis about the issue.\nb\n. \nEverything is green in console, but the networking connections are not working\ncorrectly. The \ndiagnose\n command helps to confirm that there are no other connectivity\nor deployment issues outside of the console. It is considered best practice to run the \ndiagnostics\n command after any deployment to identify issues.\nSee \ndiagnose\n in the Submariner for more information about how to run the command.\n2\n. \nIf a problem continues with the \nConnection status\n, you can start by running the \ndiagnose\ncommand of the \nsubctl\n utility tool to get a more detailed status for the connection between\ntwo Submariner clusters. The format for the command is:\nsubctl diagnose all --kubeconfig <path-to-kubeconfig-file>\nReplace \npath-to-kubeconfig-file\n with the path to the \nkubeconfig\n file. See \ndiagnose\n in the\nSubmariner documentation for more information about the command.\n3\n. \nCheck the firewall settings. Sometimes a problem with the connection is caused by firewall\npermissions issues that prevent the clusters from communicating. This can cause the \nConnection status\n to show as degraded. Run the following command to check the firewall\nissues:\nsubctl diagnose firewall inter-cluster <path-to-local-kubeconfig> <path-to-remote-cluster-\nkubeconfig>\nReplace \npath-to-local-kubeconfig\n with the path to the \nkubeconfig\n file of one of the\nclusters.\nReplace \npath-to-remote-kubeconfig\n with the path to the \nkubeconfig\n file of the other\nCHAPTER 1. TROUBLESHOOTING\n41", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2919, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b8f6538-626f-49d2-82b8-1da59bcf939d": {"__data__": {"id_": "2b8f6538-626f-49d2-82b8-1da59bcf939d", "embedding": null, "metadata": {"page_label": "46", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2322568a-0e51-42eb-8abc-ba38c2a961f6", "node_type": "4", "metadata": {"page_label": "46", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "1bc0830e88a5fcae2abf41394647fa7cd27466e19cdfa8c70949031abb4dbf87", "class_name": "RelatedNodeInfo"}}, "text": "Replace \npath-to-remote-kubeconfig\n with the path to the \nkubeconfig\n file of the other\ncluster. you can run the \nverify\n command with your \nsubctl\n utility tool to test the connection\nbetween two Submariner clusters. The basic format for the command is:\n4\n. \nIf a problem continues with the \nConnection status\n, you can run the \nverify\n command with\nyour \nsubctl\n utility tool to test the connection between two Submariner clusters. The basic\nformat for the command is:\nsubctl verify --kubecontexts <cluster1>,<cluster2> [flags]\nReplace \ncluster1\n and \ncluster2\n with the names of the clusters that you are testing. See \nverify\n in the Submariner documentation for more information about the command.\n5\n. \nAfter the troubleshooting steps resolve the issue, use the \nbenchmark\n command with the \nsubctl\n tool to establish a base on which to compare when you run additional diagnostics.\nSee \nbenchmark\n in the Submariner documentation for additional information about the\noptions for the command.\n1.33. TROUBLESHOOTING RESTORE STATUS FINISHES WITH ERRORS\nAfter you restore a backup, resources are restored correctly but the Red Hat Advanced Cluster\nManagement restore resource shows a \nFinishedWithErrors\n status.\n1.33.1. Symptom: Troubleshooting restore status finishes with errors\nRed Hat Advanced Cluster Management shows a \nFinishedWithErrors\n status and one or more of the\nVelero restore resources created by the Red Hat Advanced Cluster Management restore show a \nPartiallyFailed\n status.\n1.33.2. Resolving the problem: Troubleshooting restore status finishes with errors\nIf you restore from a backup that is empty, you can safely ignore the \nFinishedWithErrors\n status.\nRed Hat Advanced Cluster Management for Kubernetes restore shows a cumulative status for all Velero\nrestore resources. If one status is \nPartiallyFailed\n and the others are \nCompleted\n, the cumulative status\nyou see is \nPartiallyFailed\n to notify you that there is at least one issue.\nTo resolve the issue, check the status for all individual Velero restore resources with a \nPartiallyFailed\nstatus and view the logs for more details. You can get the log from the object storage directly, or\ndownload it from the OADP Operator by using the \nDownloadRequest\n custom resource.\nTo create a \nDownloadRequest\n from the console, complete the following steps:\n1\n. \nNavigate to \nOperators\n > \nInstalled Operators\n > \nCreate DownloadRequest\n.\n2\n. \nSelect \nBackupLog\n as your \nKind\n and follow the console instructions to complete the \nDownloadRequest\n creation.\n1.34. TROUBLESHOOTING MULTILINE YAML PARSING\nWhen you want to use the \nfromSecret\n function to add contents of a \nSecret\n resource into a \nRoute\nresource, the contents are displayed incorrectly.\n1.34.1. Symptom: Troubleshooting multiline YAML parsing\nRed Hat Advanced Cluster Management for Kubernetes 2.11 Troubleshooting\n42", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2854, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce20bc47-f608-4a56-9b0d-96fd35568b5b": {"__data__": {"id_": "ce20bc47-f608-4a56-9b0d-96fd35568b5b", "embedding": null, "metadata": {"page_label": "47", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fc6e9d4f-4023-4a6d-8c9f-e3524bf97d5d", "node_type": "4", "metadata": {"page_label": "47", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}, "hash": "9b85f6744aad298f00f110374bc0666af0bd927960568bed56486bd96252173b", "class_name": "RelatedNodeInfo"}}, "text": "When the managed cluster and hub cluster are the same cluster the certificate data is redacted, so the\ncontents are not parsed as a template JSON string. You might receive the following error messages:\n1.34.2. Resolving the problem: Troubleshooting multiline YAML parsing\nConfigure your certificate policy to retrieve the hub cluster and managed cluster \nfromSecret\n values.\nUse the \nautoindent\n function to update your certificate policy with the following content:\nmessage: >-\n            [spec.tls.caCertificate: Invalid value: \n\"redacted ca certificate\n            data\"\n: failed to parse CA certificate: data does not contain any\n            valid RSA or ECDSA certificates, spec.tls.certificate: Invalid\n            value: \n\"redacted certificate data\"\n: data does not contain any valid\n            RSA or ECDSA certificates, spec.tls.key: Invalid value: \n\"\"\n: no key specified]\n                 tls:\n                    certificate:\n \n|\n                      \n{{ print \"{{hub fromSecret \"open-cluster-management\" \"minio-cert\" \"tls.crt\" hub}}\n\" |\n \nbase64dec | autoindent }}\nCHAPTER 1. TROUBLESHOOTING\n43", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1109, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"9588cdb7-f2f3-4d46-af52-1f0d665b2d40": {"node_ids": ["cad615e2-1cf3-40c2-be28-65d0c21deb1c"], "metadata": {"page_label": "1", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "ad849a43-241e-400e-9bff-6627d6e6ec58": {"node_ids": ["19e616ee-a1c3-4102-aa6a-0daa99f212b0"], "metadata": {"page_label": "2", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "fcce6751-3e0e-438c-ba36-913dc9f9c41e": {"node_ids": ["22f516ba-81b7-4d30-b0b2-df07ee8cbdea"], "metadata": {"page_label": "3", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "35525858-3ccc-403d-a577-001b1a567924": {"node_ids": ["2ec500ef-febf-4cdd-8ef1-4d9c2da75abd"], "metadata": {"page_label": "4", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "0aabf701-4c83-4b6f-926f-3286a65b2419": {"node_ids": ["c6d14741-d609-4fae-8015-3b863212447f", "79685708-ae8b-45e8-be44-6dda42f18f17"], "metadata": {"page_label": "5", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "7de721a4-671c-4833-b793-889701330075": {"node_ids": ["01244469-7df9-4bce-9dbc-d53a0a3b7a09", "e8627330-4131-45ca-804c-4d3bd1567b74"], "metadata": {"page_label": "6", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "378dbcc3-837f-465f-8e96-c5732b85de38": {"node_ids": ["a8ab4073-7432-4091-9796-017b329c6ed6", "2b38aa4a-304b-4c42-94a7-11d989962f66"], "metadata": {"page_label": "7", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "d39d55fe-ee32-4125-91d1-a48c844bf232": {"node_ids": ["25148466-216d-4c24-b5e3-44153e8a90d7"], "metadata": {"page_label": "8", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "fc2fdffb-6777-43f9-b5fd-4206697c876a": {"node_ids": ["e1e03637-5d3a-4e7d-bd97-5b0ccf50e1bb"], "metadata": {"page_label": "9", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "5ee83bd6-5651-4808-91b6-3690152713da": {"node_ids": ["2b4b2e8d-520c-4b74-85e1-278d1f44c84e"], "metadata": {"page_label": "10", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "fe93d5e1-9887-4c49-a202-0e99dae0ecb6": {"node_ids": ["d240cfa0-7084-4ea0-b329-b6dd7ad61798"], "metadata": {"page_label": "11", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "25035d50-503f-4019-92e5-8d4244d9fc31": {"node_ids": ["167ac1ab-ca20-4bbd-812c-e3e47dc83686"], "metadata": {"page_label": "12", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "3fe02681-cd77-44dc-9cd3-c51406f0c13b": {"node_ids": ["cd7454dd-baec-4aef-a45e-c617c6bf8069"], "metadata": {"page_label": "13", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "9ac7f74d-d144-4bfe-b82d-a58c76ef80eb": {"node_ids": ["cdb53133-cc84-4253-9043-70dcd60c421d"], "metadata": {"page_label": "14", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "79336ac5-934d-435c-8a16-69634e5fc244": {"node_ids": ["2e752e4e-7153-48fe-88e7-73240050b054"], "metadata": {"page_label": "15", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "7576b933-dde7-4076-a724-43dbad0db089": {"node_ids": ["5c761ea2-ac52-4827-b20f-83b8ebc1ade4"], "metadata": {"page_label": "16", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "05fecf46-7952-4cf4-9978-2e61face2f01": {"node_ids": ["270ebb66-2cda-4460-8431-67c954bb91ab"], "metadata": {"page_label": "17", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "15068f1a-bf98-477c-9f82-18f02d133f53": {"node_ids": ["b0310e71-4439-45d2-8667-113f5691e793"], "metadata": {"page_label": "18", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "38466272-90c3-4529-9605-f79180422b6b": {"node_ids": ["3f41c79b-efbb-4768-b338-aae492905cd1"], "metadata": {"page_label": "19", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "6ee7fa24-2fe2-4836-8fba-62aab1276a4c": {"node_ids": ["d9d072f3-56a7-47c1-a808-4a4ae658ea26"], "metadata": {"page_label": "20", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "d383f5fe-c49c-41b8-bd0c-9c611eaf3204": {"node_ids": ["9e7773a0-93c1-4d79-82f6-d29464867d6f"], "metadata": {"page_label": "21", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "bc98ad6f-e751-4ad6-9e92-65875c0be89f": {"node_ids": ["79c2abe9-351c-4c80-990c-928442cd7fb2"], "metadata": {"page_label": "22", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "08a3bfd8-a1d3-4791-aa06-4cce0c59db37": {"node_ids": ["2495292d-46f8-4ab6-bf8c-1169511f9b7e"], "metadata": {"page_label": "23", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "210f6cf2-7cad-4c99-af9c-b15241620d9e": {"node_ids": ["0b8d2fa4-4b70-4b1e-ac25-6075d86f376a"], "metadata": {"page_label": "24", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "df687014-0152-4bb6-a745-f573a02f0c8c": {"node_ids": ["2b7ee835-ddec-44f7-98c9-89c8ac94a2e6"], "metadata": {"page_label": "25", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "29fecb31-b7ba-43fa-94f6-aae61f96e599": {"node_ids": ["753f7f61-a993-4eeb-8e71-cf99c152f5d2"], "metadata": {"page_label": "26", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "dd226be5-5385-4062-bc26-230467c3f604": {"node_ids": ["a16e535e-f070-46e7-8877-d02518b7eb91"], "metadata": {"page_label": "27", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "a9da59e6-ce38-4cdb-aba8-0ee4a3ebcf30": {"node_ids": ["9dbcbebd-4af0-486a-b73d-d9623a8b3ee2"], "metadata": {"page_label": "28", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "62d7f706-6a87-4ffb-bef9-57f43469cf87": {"node_ids": ["be8fabb0-75ab-4fd0-a051-7bfafec82d32"], "metadata": {"page_label": "29", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "1ecd8271-b8b8-4c97-aece-bf328cac961d": {"node_ids": ["eb560299-f2ae-4b86-9127-e2a946003fd4"], "metadata": {"page_label": "30", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "a5d7308c-695f-43e1-b065-e354ceadc846": {"node_ids": ["0c7ad3c2-da84-41a2-9b9e-f689560c466d"], "metadata": {"page_label": "31", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "8196e504-6b96-4f71-9900-9577678576d2": {"node_ids": ["c7602244-50b6-43af-90f9-d486e0d84232"], "metadata": {"page_label": "32", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "18d63416-8aa1-49f8-be89-a160ced2e4a8": {"node_ids": ["267d064f-3ed4-4e37-a7d2-8f01f11ee89c"], "metadata": {"page_label": "33", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "98b38f52-7ea3-47ce-90a5-8675baca361b": {"node_ids": ["35683573-feb1-444b-b76d-0364e0af53fe"], "metadata": {"page_label": "34", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "7420811d-bd3f-432c-8ddc-f5d9279985ba": {"node_ids": ["11dc20d7-a40f-4ce1-adfc-6fba5ce7badb"], "metadata": {"page_label": "35", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "38ed601c-c93d-4226-9d3c-259a036dfbf5": {"node_ids": ["8561eb5d-de5f-425d-9e98-31baea6b7d47"], "metadata": {"page_label": "36", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "62c1d69c-dc87-44d1-b4ff-98e3c659f7cc": {"node_ids": ["13e7ff9a-94d8-449f-ab2a-5d5bcff742de"], "metadata": {"page_label": "37", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "8b7630b8-439f-47ec-afa9-804e7bafe506": {"node_ids": ["e1896bc7-71d6-47be-b640-f9d50c6fa7fe"], "metadata": {"page_label": "38", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "3edc0a60-c204-4f81-a74f-f1346c63a277": {"node_ids": ["b70af8eb-46fe-4794-bb54-f2b6a1d67e93"], "metadata": {"page_label": "39", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "8b944b41-b771-4ce2-bc6b-c529f641b6a8": {"node_ids": ["0f0e8304-1fa6-402e-9ab8-fb75afdc30c7"], "metadata": {"page_label": "40", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "5590a902-cd61-4b45-b297-95b231551518": {"node_ids": ["d795628d-b898-4be1-a204-bb47093f5136"], "metadata": {"page_label": "41", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "80137291-2a05-437d-b53b-54d977631b94": {"node_ids": ["0d922663-f4db-4263-8d16-b05fdc73436c"], "metadata": {"page_label": "42", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "702be77f-6a9e-4cbb-b89c-abe017392e29": {"node_ids": ["1fb2e920-31e7-4d9e-a93d-50e7a1312a44"], "metadata": {"page_label": "43", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "dac72528-7431-413f-bdd1-421873db0bb1": {"node_ids": ["07ea2960-2d51-45aa-9840-5eb156a3b961"], "metadata": {"page_label": "44", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "d836d318-3fb1-41b4-9eb8-ac2ad3875f99": {"node_ids": ["935c7e19-54e9-4d81-b1f0-6f9e645ae0c6"], "metadata": {"page_label": "45", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "2322568a-0e51-42eb-8abc-ba38c2a961f6": {"node_ids": ["2b8f6538-626f-49d2-82b8-1da59bcf939d"], "metadata": {"page_label": "46", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}, "fc6e9d4f-4023-4a6d-8c9f-e3524bf97d5d": {"node_ids": ["ce20bc47-f608-4a56-9b0d-96fd35568b5b"], "metadata": {"page_label": "47", "file_name": "troubleshooting.pdf", "file_path": "/Users/liuwei/Workspace/hi-rag/backend/data/doc/troubleshooting.pdf", "file_type": "application/pdf", "file_size": 391684, "creation_date": "2024-08-02", "last_modified_date": "2024-07-28"}}}}